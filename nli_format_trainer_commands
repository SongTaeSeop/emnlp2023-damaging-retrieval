Input

train_file_path : 
/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1.json

dev_file_path : 
/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_dev_1.json

Output
output path : 
/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1


CUDA_VISIBLE_DEVICES=2 python binary_trainer.py \
--wandb_project binary_classifier \
--report_to wandb \
--run_name testing-binary-nq-dev-5-fold-1 \
--model_architecture roberta-large \
--model_name_or_path roberta-large \
--git_tag v1.2 \
--max_seq_length 200 \
--data binary-nq-dev-5-fold-1 \
--train_file /data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1.json \
--eval_file /data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_dev_1.json \
--num_labels 2 \
--output_dir /data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1 \
--do_train True \
--do_eval True \
--logging_steps 10 \
--save_steps 100 \
--eval_steps 100 \
--evaluation_strategy steps \
--save_total_limit 10 \
--load_best_model_at_end True \
--learning_rate 5e-5 \
--metric_for_best_model accuracy \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 16 \
--num_train_epochs 3 \
--seed 42




===========


Metric - Done
Wandb - Done

default
--overwrite_cache False
--pad_to_max_length False \

depreciated
--dev_size 3000 -> not used
--sample_size 5 -> not used
--position  1 -> not used

====================================================================


CUDA_VISIBLE_DEVICES=0,1,2,3 python trainer.py \
--output_dir /data/philhoon-relevance/KILT/kilt_nq \
--model_architecture Longformer \
--model_name_or_path allenai/longformer-base-4096 \
--git_tag v1.1 \
--data KILT_nq_dpr \
--train_file /data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-train-multikilt.json \
--dev_size 3000 \
--test_file /data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-dev-multikilt.json \
--sample_size 5 \
--position 1 \
--num_train_epochs 10 \
--learning_rate 5e-5 \
--do_train True \
--save_total_limit 5 \
--save_steps 300 \
--do_eval True \
--eval_steps 300 \
--evaluation_strategy steps \
--load_best_model_at_end True \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 8 \
--metric_for_best_model accuracy \
--seed 42

==============


generator_binary:
	PYTHONUNBUFFERED=1;COMET_API_KEY="sZwTJfHc2cfmmLVbPnhn6rlxi";CUDA_VISIBLE_DEVICES=4;TORCH_CUDA_ARCH_LIST=8.6;WANDB_DISABLED=true 


	
	
	--is_binary 1 
	--train_path ../data/train.jsonl 
	--validation_path ../data/shared_task_dev.jsonl 
	--is_test 0 
	--is_multilabel 1 
	--label_padding 96 
	--is_ranking 0 
	--is_seq_to_seq 1 
	--version_tag 1113_4 

generator_ternary:
	CUDA_VISIBLE_DEVICES=1 
	python generator.py 
	--num_train_epochs 100 
	--label_names labels 
	--output_dir binary_bert 
	--language all_languages 
	--overwrite_output_dir 
	--do_train 
	--save_total_limit 2 
	--save_steps 100 
	--evaluation_strategy steps 
	--do_eval 
	--max_eval_samples 1000 
	--load_best_model_at_end 
	--per_device_train_batch_size 8 
	--gradient_accumulation_steps 1 
	--model_name_or_path "bert-base-cased" 
	--metric_for_best_model "f1" 
	--seed 0 
	--do_predict 
	--eval_steps 100 
	--is_binary 0 
	--data_path data/shared_task_dev.jsonl
