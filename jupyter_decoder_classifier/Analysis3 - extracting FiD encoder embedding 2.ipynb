{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9908fbb",
   "metadata": {},
   "source": [
    "# Analysis3 - extracting FiD encoder embedding 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dce998",
   "metadata": {},
   "source": [
    "## CHECKING PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88dfdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "import heapq\n",
    "import pathlib\n",
    "import shutil\n",
    "from FiD.src.model import FiDT5\n",
    "from src.model import FiDEncoderForSequenceClassification\n",
    "import argparse\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from src.data import BinaryCustomDatasetShuffle\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "from util import utils\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments, CustomTrainingArguments\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import T5PreTrainedModel\n",
    "import copy\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from FiD.src.model import FiDT5\n",
    "import FiD.src.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10cd967",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d3d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Extracting encoder embeddings for passage-level decoder (GPT2)')\n",
    "\n",
    "parser.add_argument('--input_file_path', type=str, \n",
    "                    default = '/data/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1/ctx100id_split_train_1.json')\n",
    "parser.add_argument('--output_file_path', type=str, \n",
    "                    default = '/data/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1/ctx100id_embedding_train_1.pickle')\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "                    default=2)\n",
    "parser.add_argument('--n_context', type=int,\n",
    "                    default=5)\n",
    "parser.add_argument('--max_seq_length', type=int,\n",
    "                    default=200)\n",
    "parser.add_argument('--model_name_or_path', type=str,\n",
    "                    default = '/data/philhoon-relevance/FiD/pretrained_models/nq_reader_large')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab6741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6965cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53d484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = FiDT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6128151",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d576a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68b6b5b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.encoder => FiDT5.EncoderWrapper\n",
    "# model.encoder.encoder => FiDT5.EncoderWrapper.encoder = T5 encoder Architecture w FiDT5 parameters\n",
    "model_encoder = model.encoder.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5200122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-base', return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f188c2",
   "metadata": {},
   "source": [
    "## Get Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943150a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args.input_file_path)\n",
    "eval_data = args.input_file_path\n",
    "# eval_fold = '/scratch/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/ctx100id_split_train_1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53add69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1/ctx100id_split_train_1.json\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b670216",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = FiD.src.data.load_data(\n",
    "        eval_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ada771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215dfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_context = args.n_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "738aa7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(n_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90960a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = FiD.src.data.Dataset(\n",
    "    eval_examples,  \n",
    "    n_context = n_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16a1816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_maxlength = args.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e3b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_function = FiD.src.data.Collator(text_maxlength, tokenizer, n_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9d4b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampler = SequentialSampler(eval_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14c7bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_gpu_batch_size = args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ee89508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(per_gpu_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    sampler=eval_sampler, \n",
    "    batch_size=per_gpu_batch_size,\n",
    "    num_workers=8,\n",
    "    collate_fn=collator_function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "306f39a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "model_encoder.to(device = device)\n",
    "model_encoder.eval()\n",
    "check = 0\n",
    "result_dict = {}\n",
    "n_passages = args.n_context\n",
    "print(n_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4264e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6ba64cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e84fc7cb6744a0a07aa0e28777dda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([2, 3])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([4, 5])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([6, 7])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([8, 9])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([10, 11])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([12, 13])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([14, 15])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([16, 17])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([18, 19])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([20, 21])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([22, 23])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([24, 25])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([26, 27])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([28, 29])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([30, 31])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([32, 33])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([34, 35])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([36, 37])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([38, 39])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([40, 41])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([42, 43])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([44, 45])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([46, 47])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([48, 49])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([50, 51])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([52, 53])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([54, 55])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([56, 57])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([58, 59])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([60, 61])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([62, 63])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([64, 65])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([66, 67])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([68, 69])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([70, 71])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([72, 73])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([74, 75])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([76, 77])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([78, 79])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([80, 81])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([82, 83])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([84, 85])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([86, 87])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([88, 89])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([90, 91])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([92, 93])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([94, 95])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([96, 97])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([98, 99])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([100, 101])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([102, 103])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([104, 105])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([106, 107])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([108, 109])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([110, 111])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([112, 113])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([114, 115])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([116, 117])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([118, 119])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([120, 121])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([122, 123])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([124, 125])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([126, 127])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([128, 129])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([130, 131])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([132, 133])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([134, 135])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([136, 137])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([138, 139])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([140, 141])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([142, 143])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([144, 145])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([146, 147])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([148, 149])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([150, 151])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([152, 153])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([154, 155])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([156, 157])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([158, 159])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([160, 161])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([162, 163])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([164, 165])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([166, 167])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([168, 169])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([170, 171])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([172, 173])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([174, 175])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([176, 177])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([178, 179])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([180, 181])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([182, 183])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([184, 185])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([186, 187])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([188, 189])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([190, 191])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([192, 193])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([194, 195])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([196, 197])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n",
      "tensor([198, 199])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 5, 200])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 1000])\n",
      "200\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([2, 5, 200, 1024])\n"
     ]
    }
   ],
   "source": [
    "for index, _, _, passage_ids, passage_masks in tqdm(eval_dataloader):\n",
    "    print(index)\n",
    "    print(passage_ids.shape)\n",
    "    print(passage_masks.shape)\n",
    "    \n",
    "    input_ids = passage_ids.view(passage_ids.size(0), -1)\n",
    "    attention_mask = passage_masks.view(passage_masks.size(0), -1)\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask.shape)\n",
    "    \n",
    "    bsz, total_length = input_ids.shape\n",
    "    passage_length = total_length // n_passages\n",
    "    print(passage_length)\n",
    "    \n",
    "    input_ids = input_ids.view(bsz*n_passages, passage_length)\n",
    "    print(input_ids.shape)\n",
    "    \n",
    "    attention_mask = attention_mask.view(bsz*n_passages, passage_length)\n",
    "    print(attention_mask.shape)\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    outputs = model_encoder(input_ids, attention_mask)\n",
    "    output_by_batch = outputs[0]\n",
    "    output_by_batch = output_by_batch.view(bsz, n_passages, passage_length, -1)\n",
    "    output_by = output_by_batch.detach().cpu()\n",
    "    \n",
    "    print(output_by.shape)\n",
    "    for i in range(bsz):\n",
    "        id_ = index[i].item()\n",
    "        embedding = output_by[i,]\n",
    "        result_dict[id_] = embedding\n",
    "    \n",
    "    check += 1\n",
    "    if check == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc6be82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 1024])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d5eb5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 1024])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a23f0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 1024])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df384080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict[150].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02ee947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1/ctx100id_embedding_train_1.pickle\n"
     ]
    }
   ],
   "source": [
    "print(args.output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.output_file_path, 'wb') as f:\n",
    "    pickle.dump(new_instance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22294ba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.7715e-02,  8.5688e-02, -4.5739e-02,  ..., -2.1708e-01,\n",
       "          -9.0992e-02, -9.1965e-03],\n",
       "         [-1.2303e-01,  1.3432e-01, -1.2805e-04,  ..., -2.7478e-01,\n",
       "          -2.6327e-02,  4.0313e-02],\n",
       "         [ 1.5508e-01,  4.1932e-02,  1.4576e-01,  ...,  4.5006e-01,\n",
       "          -2.3713e-01, -1.4170e-01],\n",
       "         ...,\n",
       "         [ 4.0225e-01, -1.3431e-01,  3.1150e-01,  ..., -1.3394e-01,\n",
       "          -2.4641e-01,  3.5341e-01],\n",
       "         [ 3.8914e-01, -1.2264e-01,  3.1169e-01,  ..., -1.4295e-01,\n",
       "          -2.5223e-01,  3.7069e-01],\n",
       "         [ 3.8478e-01, -1.0816e-01,  3.2307e-01,  ..., -1.5112e-01,\n",
       "          -2.5433e-01,  3.6048e-01]],\n",
       "\n",
       "        [[-1.5032e-01,  6.8224e-02, -2.6341e-02,  ..., -2.2828e-01,\n",
       "          -7.3244e-02, -2.0166e-03],\n",
       "         [-1.9287e-01,  1.3305e-01,  3.4472e-02,  ..., -2.6626e-01,\n",
       "          -2.9993e-02,  5.5405e-02],\n",
       "         [ 1.4436e-02,  4.2459e-02,  2.2709e-01,  ...,  3.7087e-01,\n",
       "          -2.3722e-01, -1.4227e-01],\n",
       "         ...,\n",
       "         [ 4.4441e-01,  9.4947e-02,  2.4245e-01,  ..., -7.0115e-02,\n",
       "          -4.1631e-02,  3.8703e-01],\n",
       "         [ 4.6392e-01,  9.2197e-02,  2.2987e-01,  ..., -9.6557e-02,\n",
       "          -4.5630e-02,  3.9671e-01],\n",
       "         [ 4.8398e-01,  1.0232e-01,  2.2304e-01,  ..., -7.9725e-02,\n",
       "          -3.4787e-02,  3.9458e-01]],\n",
       "\n",
       "        [[-1.6666e-01,  8.1212e-02, -7.5911e-02,  ..., -2.1275e-01,\n",
       "          -1.2251e-01, -3.2915e-02],\n",
       "         [-2.1621e-01,  1.3083e-01, -2.1734e-02,  ..., -2.4545e-01,\n",
       "          -7.4445e-02,  4.1933e-02],\n",
       "         [ 5.4596e-02, -6.2025e-03,  2.2146e-01,  ...,  3.5769e-01,\n",
       "          -2.3839e-01, -6.3211e-02],\n",
       "         ...,\n",
       "         [ 3.8040e-01, -1.4002e-02,  2.4503e-01,  ..., -2.0032e-02,\n",
       "          -2.1083e-01,  2.9898e-01],\n",
       "         [ 3.9118e-01, -1.7072e-02,  2.4282e-01,  ..., -1.2490e-02,\n",
       "          -2.0535e-01,  3.0938e-01],\n",
       "         [ 4.0592e-01, -1.2933e-02,  2.3536e-01,  ..., -1.5986e-02,\n",
       "          -2.1605e-01,  3.0442e-01]],\n",
       "\n",
       "        [[-1.4015e-01,  6.7180e-02, -2.7944e-02,  ..., -2.2257e-01,\n",
       "          -5.6920e-02, -6.9358e-05],\n",
       "         [-1.8422e-01,  1.2517e-01,  3.1629e-02,  ..., -2.6100e-01,\n",
       "          -1.8734e-02,  5.9999e-02],\n",
       "         [ 4.5034e-02, -1.2561e-03,  2.1019e-01,  ...,  3.7672e-01,\n",
       "          -2.2706e-01, -1.1723e-01],\n",
       "         ...,\n",
       "         [ 6.8403e-01, -4.7883e-02,  1.0873e-01,  ...,  3.3232e-02,\n",
       "          -1.7592e-01,  4.3201e-01],\n",
       "         [ 6.8917e-01, -4.0568e-02,  1.0595e-01,  ...,  1.2833e-02,\n",
       "          -1.7114e-01,  4.4686e-01],\n",
       "         [ 6.6632e-01, -2.9103e-02,  1.2469e-01,  ..., -2.9473e-02,\n",
       "          -1.8590e-01,  4.5758e-01]],\n",
       "\n",
       "        [[-1.6108e-01,  8.0367e-02, -5.9383e-02,  ..., -2.4159e-01,\n",
       "          -1.2436e-01, -5.5904e-02],\n",
       "         [-2.1157e-01,  1.3164e-01, -4.4031e-03,  ..., -2.7772e-01,\n",
       "          -7.8756e-02,  2.1077e-02],\n",
       "         [ 7.5726e-02, -1.1716e-02,  2.0732e-01,  ...,  3.1939e-01,\n",
       "          -2.4664e-01, -5.5799e-02],\n",
       "         ...,\n",
       "         [ 3.6753e-01, -1.2200e-01,  1.7991e-01,  ...,  5.0819e-02,\n",
       "          -6.7099e-02,  2.5723e-01],\n",
       "         [ 3.5885e-01, -1.0433e-01,  1.8842e-01,  ...,  3.3229e-02,\n",
       "          -7.7643e-02,  2.5499e-01],\n",
       "         [ 3.3907e-01, -8.8685e-02,  2.0014e-01,  ...,  3.8424e-02,\n",
       "          -9.3340e-02,  2.6310e-01]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902261d",
   "metadata": {},
   "source": [
    "### Chekc result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8186e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c9da10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1/ctx100id_embedding_train_1.pickle'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97fac558",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.output_file_path, 'rb') as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a30cebd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fa14204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 200, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620c66e",
   "metadata": {},
   "source": [
    "## Changing Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead28994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathlib \n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e922b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_path = '/scratch/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5925423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = pathlib.Path(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d2d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = path.glob('**/*')\n",
    "\n",
    "# test_file = list(files)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039a3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82435f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(test_file.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98aa418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = test_file.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2038a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62fa6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = 'ctx100id_embedding_train_1.picklectx100id_*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6bea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.match(reg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07f1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = filename.split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "500a43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ctx100id_embedding_train_1' in filename\n",
    "\n",
    "# new_file_name = 'ctx100id_embedding_train_1' + f'_{num}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3490d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d63f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous = current_path + '/' + filename + '.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06fc2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab33dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = current_path + '/' + new_file_name + '.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c6703b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc380f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.rename(previous, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3262a",
   "metadata": {},
   "source": [
    "## Testing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87b2ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9bba52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/scratch/philhoon-relevance/decoder-classification/NQ-DEV-DPR/5-fold/1/ctx100id_embedding_train_1_1.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb33f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3b15b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b67b35d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 200, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[7]['embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0b8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3f6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fc622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495870e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
