{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2974c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, T5EncoderModel\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c8683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model option\n",
    "# roberta-large, t5-large\n",
    "model_option = 't5-large' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e73bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philhoon/miniconda3/envs/relevance-kilt/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.final_layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_option)\n",
    "\n",
    "if 't5' in model_option:\n",
    "    model = T5EncoderModel.from_pretrained(model_option)\n",
    "else:\n",
    "    model = AutoModel.from_pretrained(model_option)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2328250",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Belfast\", \"London\", \"Paris\", \"Berlin\", \"Madrid\", \"Dublin\", \"Lisbon\", \"Rome\", \"Moscow\", \"New Delhi\", \"Seoul\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc047beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 't5' in model_option:\n",
    "    encoded_input = tokenizer(cities, return_tensors='pt', padding=True)\n",
    "else:\n",
    "    encoded_input = tokenizer(cities, return_tensors='pt', padding=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a9faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41aa78f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "988848e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_similarity(last_hidden_layer):\n",
    "#     cls_dim = last_hidden_layer[:,0,:]\n",
    "#     a_norm = cls_dim / cls_dim.norm(dim=1)[:, None]\n",
    "#     b_norm = cls_dim / cls_dim.norm(dim=1)[:, None]\n",
    "#     res = torch.mm(a_norm, b_norm.transpose(0,1))\n",
    "    \n",
    "#     return res\n",
    "\n",
    "# def dot_similarity(last_hidden_layer):\n",
    "#     cls_dim = last_hidden_layer[:,0,:]\n",
    "#     a_norm = cls_dim\n",
    "#     b_norm = cls_dim\n",
    "#     res = torch.mm(a_norm, b_norm.transpose(0,1))\n",
    "    \n",
    "#     return res\n",
    "\n",
    "def get_smilarity(last_hidden_layer, option='consine'):\n",
    "    cls_dim = last_hidden_layer[:,0,:]\n",
    "    if option == 'consine':\n",
    "        a_norm = cls_dim / cls_dim.norm(dim=1)[:, None]\n",
    "        b_norm = cls_dim / cls_dim.norm(dim=1)[:, None]\n",
    "    else:\n",
    "        a_norm = cls_dim\n",
    "        b_norm = cls_dim\n",
    "        \n",
    "    return torch.mm(a_norm, b_norm.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80e9b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option for similarity\n",
    "# 'consine', 'dot'\n",
    "similarity = get_smilarity(output.last_hidden_state, option='consine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea6f7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e11205c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = similarity.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e284a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adebf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "[1.0000001, 0.46194863, 0.40122962, 0.41659728, 0.2945966, 0.4964512, 0.4321043, 0.2867211, 0.38526952, 0.18646638, 0.39866138],\n",
      "[0.46194863, 1.0000002, 0.72997266, 0.7026277, 0.57633257, 0.793318, 0.7777086, 0.5779804, 0.70575714, 0.3023165, 0.6565156],\n",
      "[0.40122962, 0.72997266, 0.99999994, 0.73374236, 0.70930564, 0.7330529, 0.71778286, 0.67648643, 0.7167645, 0.30729184, 0.70543003],\n",
      "[0.41659728, 0.7026277, 0.73374236, 0.9999999, 0.6328578, 0.6856353, 0.680172, 0.6389555, 0.7365085, 0.4158886, 0.69414234],\n",
      "[0.2945966, 0.57633257, 0.70930564, 0.6328578, 1.0000001, 0.59341633, 0.6930702, 0.6433421, 0.65286577, 0.24420226, 0.62368095],\n",
      "[0.4964512, 0.793318, 0.7330529, 0.6856353, 0.59341633, 1.0000002, 0.7765486, 0.5444765, 0.6986886, 0.35437268, 0.682055],\n",
      "[0.4321043, 0.7777086, 0.71778286, 0.680172, 0.6930702, 0.7765486, 0.9999998, 0.615153, 0.72757876, 0.26245904, 0.66718864],\n",
      "[0.2867211, 0.5779804, 0.67648643, 0.6389555, 0.6433421, 0.5444765, 0.615153, 1.0, 0.65519696, 0.2634263, 0.56046206],\n",
      "[0.38526952, 0.7057569, 0.7167646, 0.7365086, 0.6528659, 0.69868845, 0.72757876, 0.655197, 0.9999999, 0.30375308, 0.72026247],\n",
      "[0.1864664, 0.3023165, 0.30729187, 0.41588867, 0.24420229, 0.3543727, 0.2624591, 0.2634263, 0.30375308, 0.99999994, 0.34796184],\n",
      "[0.39866138, 0.6565156, 0.70543, 0.69414246, 0.62368107, 0.6820551, 0.66718876, 0.56046206, 0.72026247, 0.34796184, 1.0],\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# for copy and paste\n",
    "print('[')\n",
    "for row in a:\n",
    "    out = ', '.join([str(value) for value in row])\n",
    "    out = '[' + out + '],'\n",
    "    print(out)\n",
    "print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = [\n",
    "[1003.59644, 1003.1952, 1003.17896, 1003.2844, 1003.25244, 1003.3141, 1003.18945, 1003.1713, 1003.128, 1003.2482, 1003.3139],\n",
    "[1003.1952, 1003.54205, 1003.4163, 1003.29974, 1003.26624, 1003.2899, 1003.2155, 1003.1977, 1003.3356, 1003.32477, 1003.3151],\n",
    "[1003.17896, 1003.4163, 1003.5079, 1003.3141, 1003.27203, 1003.2915, 1003.2136, 1003.2303, 1003.31757, 1003.2936, 1003.2991],\n",
    "[1003.2844, 1003.29974, 1003.3141, 1003.54095, 1003.32935, 1003.416, 1003.23773, 1003.2526, 1003.23535, 1003.32446, 1003.37195],\n",
    "[1003.25244, 1003.26624, 1003.27203, 1003.32935, 1003.5245, 1003.33246, 1003.27203, 1003.2493, 1003.1965, 1003.32025, 1003.3255],\n",
    "[1003.3141, 1003.2899, 1003.2915, 1003.416, 1003.33246, 1003.56146, 1003.21686, 1003.24695, 1003.2071, 1003.376, 1003.38165],\n",
    "[1003.18945, 1003.2155, 1003.2136, 1003.23773, 1003.27203, 1003.21686, 1003.53204, 1003.1882, 1003.0486, 1003.18634, 1003.1883],\n",
    "[1003.1713, 1003.1977, 1003.2303, 1003.2526, 1003.2493, 1003.24695, 1003.1882, 1003.5011, 1003.13947, 1003.2319, 1003.2338],\n",
    "[1003.128, 1003.3357, 1003.31775, 1003.2354, 1003.1963, 1003.2071, 1003.0486, 1003.1395, 1003.49097, 1003.2679, 1003.24945],\n",
    "[1003.2482, 1003.325, 1003.2936, 1003.3245, 1003.32007, 1003.3758, 1003.18634, 1003.23157, 1003.2679, 1003.57056, 1003.3897],\n",
    "[1003.31384, 1003.3151, 1003.2992, 1003.372, 1003.3257, 1003.3816, 1003.18823, 1003.2339, 1003.24945, 1003.3897, 1003.5754],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba33cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7549764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
