{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9908fbb",
   "metadata": {},
   "source": [
    "# modeling14-5-FiD-encoder-sentence-level-classifier-from-scratch\n",
    "- modeling sentence-classifier\n",
    "- FiD-encoder\n",
    "- python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938b3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97948bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7c74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "import heapq\n",
    "import pickle\n",
    "import pathlib\n",
    "import shutil\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from src.data import (\n",
    "    BinaryCustomDatasetShuffle,\n",
    "    BinarySentenceDataset,\n",
    "    BinaryCustomDatasetDecisiveBinaryGold,\n",
    "    BinaryCustomDatasetPredictionShuffle,\n",
    "    SentenceClassificationDataset,\n",
    "    EncoderSentenceClassificationDataset\n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "from util import utils\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments, CustomTrainingArguments\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from FiD.src.model import FiDT5\n",
    "from src.model import SentenceLSTM\n",
    "\n",
    "NEW_LINE = \"\\n\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "DATASET_MAPPING = {\n",
    "    \"BinaryCustomDatasetShuffle\" : BinaryCustomDatasetShuffle,\n",
    "    \"BinarySentenceDataset\" : BinarySentenceDataset,\n",
    "    'BinaryCustomDatasetDecisiveBinaryGold' : BinaryCustomDatasetDecisiveBinaryGold,\n",
    "    'BinaryCustomDatasetPredictionShuffle' : BinaryCustomDatasetPredictionShuffle,\n",
    "    'SentenceClassificationDataset' : SentenceClassificationDataset,\n",
    "    'EncoderSentenceClassificationDataset' : EncoderSentenceClassificationDataset\n",
    "}\n",
    "EMBEDDING_ARC_MAPPING = {\n",
    "    \"SentenceTransformer\" : SentenceTransformer,\n",
    "     \"FiDT5\" : FiDT5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3f94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, CustomTrainingArguments)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac14362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args, data_args, train_args = parser.parse_args_into_dataclasses([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args.with_tracking = True\n",
    "train_args.report_to = 'wandb'\n",
    "train_args.wandb_project = 'sequence_classifier'\n",
    "train_args.run_name = 'TESTING-FiD-Encoder-lstm-sequence_exclude_no_answer_exclude_indecisve-test'\n",
    "train_args.output_dir = '/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1/TESTING-FiD-Encoder-lstm-sequence_exclude_no_answer_exclude_indecisve-test'\n",
    "train_args.seed = 42\n",
    "train_args.num_layers = 2\n",
    "train_args.drop_out_rate = 0.2\n",
    "train_args.padding = -100\n",
    "train_args.per_device_train_batch_size = 2\n",
    "train_args.checkpointing_steps = '10'\n",
    "train_args.num_train_epochs = 100\n",
    "train_args.best_metric = 'f1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d730f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e-05\n",
      "0.9\n",
      "0.999\n",
      "1e-08\n",
      "1\n",
      "linear\n",
      "0\n",
      "False\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_args.learning_rate)\n",
    "print(train_args.adam_beta1)\n",
    "print(train_args.adam_beta2)\n",
    "print(train_args.adam_epsilon)\n",
    "print(train_args.gradient_accumulation_steps)\n",
    "print(train_args.lr_scheduler_type)\n",
    "print(train_args.num_warmup_steps)\n",
    "# print(train_args.max_train_steps)\n",
    "print(train_args.class_weights)\n",
    "print(train_args.train_loss_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b095ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.embedding = 1024\n",
    "model_args.max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047cf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args.model_architecture = \"SentenceTransformer\"\n",
    "# model_args.model_name_or_path = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc829f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.train_file = '/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/sequence_exclude_no_answer_exclude_indecisve/testing-sequence_exclude_no_answer_exclude_indecisve_ctx100id_split_train_1.pickle'\n",
    "data_args.eval_file = '/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/sequence_exclude_no_answer_exclude_indecisve/testing-sequence_exclude_no_answer_exclude_indecisve_ctx100id_split_train_1.pickle'\n",
    "data_args.dataset_class = 'EncoderSentenceClassificationDataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb50f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)\n",
    "\n",
    "accelerator = (\n",
    "    Accelerator(log_with=train_args.report_to, logging_dir=train_args.output_dir) if train_args.with_tracking else Accelerator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f468628",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d037f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/08/2023 22:44:40 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "if train_args.seed is not None:\n",
    "    set_seed(train_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ccff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process and train_args.output_dir is not None:\n",
    "    os.makedirs(train_args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a089b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1/TESTING-FiD-Encoder-lstm-sequence_exclude_no_answer_exclude_indecisve-test'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7bd411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_args.model_architecture in EMBEDDING_ARC_MAPPING:\n",
    "#     embedding_model = EMBEDDING_ARC_MAPPING[model_args.model_architecture](model_args.model_name_or_path)\n",
    "#     model_args.embedding = 384\n",
    "#     model_args.max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e68bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be936112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceLSTM(num_layers = train_args.num_layers, \n",
    "                     embedding_size = model_args.embedding, \n",
    "                     num_labels = data_args.num_labels,\n",
    "                     drop_out_rate = train_args.drop_out_rate\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1cf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = data_args.train_file\n",
    "eval_file = data_args.eval_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd646d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_file, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "with open(eval_file, 'rb') as f:\n",
    "    eval_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = utils.open_json(train_file)\n",
    "# eval_data = utils.open_json(eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_train_data = utils.prepare_sequential_data(train_data)\n",
    "# seq_eval_data = utils.prepare_sequential_data(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSetClass = DATASET_MAPPING[data_args.dataset_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a158e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffled Here\n",
    "train_dataset = EncoderSentenceClassificationDataset(train_data)\n",
    "eval_dataset = EncoderSentenceClassificationDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d690b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in random.sample(range(len(train_dataset)), 5):\n",
    "#     logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch, padding):\n",
    "    train_lst = [b['input_embedding'] for b in batch]\n",
    "    label_lst = [b['em_pattern'] for b in batch]\n",
    "    seq_len_lst = [b['em_pattern'].shape[0] for b in batch]\n",
    "    max_seq_len = max(seq_len_lst)\n",
    "    \n",
    "    padding_train_lst = []\n",
    "    for embedding in train_lst:\n",
    "        if embedding.shape[0] < max_seq_len:\n",
    "            post_pad = torch.full(size=(max_seq_len-embedding.shape[0], embedding.shape[1]), fill_value = padding)\n",
    "            post_pad = torch.full(size=(max_seq_len-embedding.shape[0], embedding.shape[1]), fill_value = -100)\n",
    "            padding_train_lst.append(torch.concat([embedding, post_pad]))\n",
    "        else:\n",
    "            padding_train_lst.append(embedding)\n",
    "            \n",
    "    inputs = torch.stack(padding_train_lst)\n",
    "    \n",
    "    padding_label_lst = []\n",
    "    for label in label_lst:\n",
    "        if label.shape[0] < max_seq_len:\n",
    "            post_pad = torch.full(size=(max_seq_len-label.shape[0], ), fill_value = padding)\n",
    "            post_pad = torch.full(size=(max_seq_len-label.shape[0], ), fill_value = -100)\n",
    "            torch.concat([label, post_pad])\n",
    "            padding_label_lst.append(torch.concat([label, post_pad]))\n",
    "        else:\n",
    "            padding_label_lst.append(label)\n",
    "            \n",
    "    labels = torch.stack(padding_label_lst)\n",
    "    \n",
    "    return {\n",
    "        'inputs' : inputs,\n",
    "        'labels' : labels,\n",
    "        'sequence_len' : torch.tensor(seq_len_lst)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                          shuffle=False,\n",
    "                              collate_fn= partial(custom_collate, padding = train_args.padding),\n",
    "                              batch_size=train_args.per_device_train_batch_size,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                              shuffle = False,\n",
    "                              collate_fn= partial(custom_collate, padding = train_args.padding),\n",
    "                              batch_size=train_args.per_device_eval_batch_size,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": train_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d84c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters,\n",
    "                              lr=train_args.learning_rate,\n",
    "                              betas=(train_args.adam_beta1, train_args.adam_beta2),\n",
    "                              eps=train_args.adam_epsilon,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6611e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a77691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / train_args.gradient_accumulation_steps)\n",
    "if train_args.max_train_steps is None:\n",
    "    train_args.max_train_steps = train_args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    name=train_args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=train_args.num_warmup_steps,\n",
    "    num_training_steps=train_args.max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e46e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0df694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / train_args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    train_args.max_train_steps = train_args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "train_args.num_train_epochs = math.ceil(train_args.max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps = train_args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe35d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_args.with_tracking:\n",
    "    experiment_config = vars(train_args)\n",
    "\n",
    "    accelerator.init_trackers(train_args.wandb_project, config=experiment_config,\n",
    "                              init_kwargs={\"wandb\": {\"name\": train_args.run_name}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_pre = evaluate.load('precision')\n",
    "metric_re = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b71c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "total_batch_size = train_args.per_device_train_batch_size * accelerator.num_processes * train_args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {train_args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {train_args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {train_args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {train_args.max_train_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_args, data_args, train_args\n",
    "train_dict = vars(train_args)\n",
    "logger.info(f\"  Saving training_args = {train_dict}\")\n",
    "with open(os.path.join(train_args.output_dir, f\"train_args.json\"), \"w\") as f:\n",
    "    json.dump(train_dict, f)\n",
    "\n",
    "model_dict = vars(model_args)\n",
    "logger.info(f\"  Saving model_args = {model_dict}\")\n",
    "with open(os.path.join(train_args.output_dir, f\"model_args.json\"), \"w\") as f:\n",
    "    json.dump(model_dict, f)\n",
    "\n",
    "data_dict = vars(data_args)\n",
    "logger.info(f\"  Saving data_args = {data_dict}\")\n",
    "with open(os.path.join(train_args.output_dir, f\"data_args.json\"), \"w\") as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(train_args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# Using heap for limiting number of saved models\n",
    "model_heap = []\n",
    "heapq.heapify(model_heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, eval_dataloader, accelerator, metric_acc, metric_pre, metric_re, metric_f1, \n",
    "         train_args, epoch, steps, output_dir, logger):\n",
    "\n",
    "    eval_progress_bar = tqdm(range(len(eval_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    prediction_lst = []\n",
    "    reference_lst = []\n",
    "\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch['inputs'], batch['sequence_len'])\n",
    "            if train_args.class_weights:\n",
    "                criterion = torch.nn.CrossEntropyLoss(weight=class_weights, reduction='mean', ignore_index=-100).cuda()\n",
    "            else:\n",
    "                criterion = torch.nn.CrossEntropyLoss(ignore_index=-100).cuda() \n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), batch['labels'].view(-1))\n",
    "        \n",
    "        if train_args.with_tracking:\n",
    "            eval_loss += loss.detach().float()\n",
    "\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        references = batch['labels']\n",
    "        \n",
    "        # Get mask for target values != padding index\n",
    "        nonpad_mask = references != train_args.padding\n",
    "        \n",
    "        # Slice out non-pad values\n",
    "        references = references[nonpad_mask]\n",
    "        predictions = predictions[nonpad_mask]\n",
    "        \n",
    "        predictions, references = accelerator.gather((predictions, references))\n",
    "        # If we are in a multiprocess environment, the last batch has duplicates\n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "\n",
    "        metric_acc.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "        metric_pre.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "        metric_re.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "        metric_f1.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "        eval_progress_bar.update(1)\n",
    "        prediction_lst.extend(predictions.detach().cpu().tolist())\n",
    "        reference_lst.extend(references.detach().cpu().tolist())\n",
    "\n",
    "    eval_metric = metric_acc.compute()\n",
    "    eval_metric_pre = metric_pre.compute()\n",
    "    eval_metric_re = metric_re.compute()\n",
    "    eval_metric_f1 = metric_f1.compute()\n",
    "\n",
    "    logger.info(f\"Evaluation at Epoch : {epoch} Total Step : {steps}\")\n",
    "    logger.info(f\"Accuracy : {eval_metric['accuracy']} Precision : {eval_metric_pre['precision']}\")\n",
    "    logger.info(f\"Recall : {eval_metric_re['recall']} F1 : {eval_metric_f1['f1']}\")\n",
    "    logger.info(f\"Epoch : {epoch} Step : {steps}\")\n",
    "    logger.info(f\"Eval_loss : {eval_loss.item() / len(eval_dataloader)}\")\n",
    "\n",
    "    result_log = {\n",
    "        \"eval_accuracy\": eval_metric['accuracy'],\n",
    "        \"eval_precision\": eval_metric_pre['precision'],\n",
    "        \"eval_recall\": eval_metric_re['recall'],\n",
    "        \"eval_f1\": eval_metric_f1['f1'],\n",
    "        \"eval_loss\": eval_loss.item() / len(eval_dataloader),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": steps,\n",
    "    }\n",
    "\n",
    "    output_result_path = os.path.join(output_dir, f\"epoch{epoch}_steps{steps}_results.json\")\n",
    "    with open(output_result_path, \"w\") as f:\n",
    "        json.dump(result_log, f)\n",
    "\n",
    "    if train_args.with_tracking:\n",
    "        accelerator.log(\n",
    "            result_log,\n",
    "            step=steps,\n",
    "        )\n",
    "\n",
    "    ## Extra\n",
    "    prediction_np = np.array(prediction_lst)\n",
    "    reference_np = np.array(reference_lst)\n",
    "    y_actu = pd.Series(reference_np, name='Actual')\n",
    "    y_pred = pd.Series(prediction_np, name='Predicted')\n",
    "\n",
    "    reversey_pred = y_pred.map(lambda x: 0 if x == 1 else 1)\n",
    "    reversey_actu = y_actu.map(lambda x: 0 if x == 1 else 1)\n",
    "    rev_accuracy = accuracy_score(reversey_actu, reversey_pred)\n",
    "    rev_precision = precision_score(reversey_actu, reversey_pred)\n",
    "    rev_recall = recall_score(reversey_actu, reversey_pred)\n",
    "    rev_f1 = f1_score(reversey_actu, reversey_pred)\n",
    "\n",
    "    logger.info(f\"rev Evaluation at Epoch : {epoch} Total Step : {steps}\")\n",
    "    logger.info(f\"rev_Accuracy : {rev_accuracy} rev_Precision : {rev_precision}\")\n",
    "    logger.info(f\"rev_Recall : {rev_recall} rev_F1 : {rev_f1}\")\n",
    "    logger.info(f\"Epoch : {epoch} Step : {steps}\")\n",
    "    logger.info(f\"Eval_loss : {eval_loss.item() / len(eval_dataloader)}\")\n",
    "\n",
    "    result_rev_log = {\n",
    "        \"eval_rev_accuracy\": rev_accuracy,\n",
    "        \"eval_rev_precision\": rev_precision,\n",
    "        \"eval_rev_recall\": rev_recall,\n",
    "        \"eval_rev_f1\": rev_f1,\n",
    "        \"eval_loss\": eval_loss.item() / len(eval_dataloader),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": steps,\n",
    "    }\n",
    "\n",
    "    output_result_path = os.path.join(output_dir, f\"epoch{epoch}_steps{steps}_rev_results.json\")\n",
    "    with open(output_result_path, \"w\") as f:\n",
    "        json.dump(result_rev_log, f)\n",
    "\n",
    "    if train_args.with_tracking:\n",
    "        accelerator.log(\n",
    "            result_rev_log,\n",
    "            step=steps,\n",
    "        )\n",
    "\n",
    "    return result_log, output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0c0e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(starting_epoch, train_args.num_train_epochs):\n",
    "    model.train()\n",
    "    if train_args.with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        logits = model(batch['inputs'], batch['sequence_len'])\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=-100).cuda() \n",
    "            \n",
    "        loss = criterion(logits.view(-1, logits.shape[-1]), batch['labels'].view(-1))\n",
    "\n",
    "        # We keep track of the loss at each epoch\n",
    "        if train_args.with_tracking:\n",
    "            cur_loss = loss.detach().float()\n",
    "            total_loss += cur_loss\n",
    "\n",
    "        loss = loss / train_args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if step % train_args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if completed_steps % train_args.train_loss_steps == 0 and step % train_args.gradient_accumulation_steps == 0:\n",
    "            logger.info(f\"Train loss {cur_loss} at current step  {completed_steps}\")\n",
    "            train_loss_log = {\n",
    "                \"train_loss\": cur_loss,\n",
    "                \"step\": completed_steps,\n",
    "            }\n",
    "            if train_args.with_tracking:\n",
    "                accelerator.log(\n",
    "                    train_loss_log,\n",
    "                    step=completed_steps,\n",
    "                )\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0 and step % train_args.gradient_accumulation_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if train_args.output_dir is not None:\n",
    "                    output_dir = os.path.join(train_args.output_dir, output_dir)\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                result_log, model_output_path = eval(model, eval_dataloader, accelerator, \n",
    "                                                     metric_acc, metric_pre, metric_re, metric_f1, \n",
    "                                                     train_args, epoch, completed_steps, output_dir, \n",
    "                                                     logger)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "                key_best_metric = f'eval_{train_args.best_metric}'\n",
    "                best_metric = result_log[key_best_metric]\n",
    "                logger.info(f\"best_metric : {best_metric}\")\n",
    "                heapq.heappush(model_heap, (best_metric, completed_steps, result_log, model_output_path))\n",
    "\n",
    "                if len(model_heap) > train_args.save_max_limit:\n",
    "                    _, _, _ ,delete_path = heapq.heappop(model_heap)\n",
    "                    logger.info(f\"Deleting file for path : {delete_path}\")\n",
    "                    mydir = pathlib.Path(delete_path)\n",
    "                    shutil.rmtree(mydir)\n",
    "                model.train()\n",
    "\n",
    "        if completed_steps >= train_args.max_train_steps:\n",
    "            break\n",
    "    \n",
    "    output_dir = f\"epoch_{epoch}_step_{completed_steps}\"\n",
    "    if train_args.output_dir is not None:\n",
    "        output_dir = os.path.join(train_args.output_dir, output_dir)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    result_log, model_output_path = eval(model, eval_dataloader, accelerator, \n",
    "                                         metric_acc, metric_pre, metric_re, metric_f1, \n",
    "                                         train_args, epoch, completed_steps, output_dir, \n",
    "                                         logger)\n",
    "    accelerator.save_state(output_dir)\n",
    "\n",
    "    key_best_metric = f'eval_{train_args.best_metric}'\n",
    "    best_metric = result_log[key_best_metric]\n",
    "    logger.info(f\"best_metric : {best_metric}\")\n",
    "    heapq.heappush(model_heap, (best_metric, completed_steps, result_log, model_output_path))\n",
    "\n",
    "    if len(model_heap) > train_args.save_max_limit:\n",
    "        _, _, _ ,delete_path = heapq.heappop(model_heap)\n",
    "        logger.info(f\"Deleting file for path : {delete_path}\")\n",
    "        mydir = pathlib.Path(delete_path)\n",
    "        shutil.rmtree(mydir)\n",
    "            \n",
    "if train_args.with_tracking:\n",
    "    accelerator.end_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c1132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428f7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf81b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0f2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b2659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab9afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0927e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ed39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2b2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e125e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11528f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782523b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vars(train_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42199a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41903e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f85ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e14c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e8162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb4027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc3f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdf488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f412c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776df27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.open_json(train_file)\n",
    "eval_data = utils.open_json(eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fde6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec963609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BinaryCustomDatasetShuffle(train_data, tokenizer = tokenizer, \\\n",
    "                                           max_length = max_length, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = BinaryCustomDatasetShuffle(eval_data, tokenizer = tokenizer, \\\n",
    "                                           max_length = max_length, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47026d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e92816",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle = True,\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                              shuffle = True,\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a77035",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2a0d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters[0][\"weight_decay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba674bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps = 0\n",
    "# max_train_steps = \n",
    "num_train_epochs = 5\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81266a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8961a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d33b38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = per_device_train_batch_size * accelerator.num_processes * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c96172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b459680",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch = 0\n",
    "with_tracking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25897061",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    if with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        if with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "            \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            \n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps }\"\n",
    "                if output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "                \n",
    "                \n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "         with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) \n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        \n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "        \n",
    "        metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "        \n",
    "        eval_metric = metric.compute()\n",
    "        logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "        \n",
    "        if args.with_tracking:\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"accuracy\" : eval_metric,\n",
    "                    \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": completed_steps,\n",
    "                },\n",
    "                step=completed_steps,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_pre = evaluate.load('precision')\n",
    "metric_re = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18636963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7522dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "args = [\"--model_name_or_path\", 'allenai/longformer-large-4096', '--output_dir', './']\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b309f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f31db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=model_args.num_labels,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.train_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    train_instance = instances[data_args.dev_size:]\n",
    "    dev_instance = instances[:data_args.dev_size]\n",
    "    \n",
    "    train_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    dev_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.test_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    test_dataset = CustomDataset(instances, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"xnli\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize Trainer\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer, \n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_train else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7e951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.dataset_name = a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed523bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e978cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
