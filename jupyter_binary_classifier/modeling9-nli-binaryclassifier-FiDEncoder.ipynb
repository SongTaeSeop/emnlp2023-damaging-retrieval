{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9908fbb",
   "metadata": {},
   "source": [
    "# modeling9-nli-binaryclassifier-FiDEncoder\n",
    "- Checking parser\n",
    "- modeling based on FiDEncoder\n",
    "- checking metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dce998",
   "metadata": {},
   "source": [
    "## CHECKING PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88dfdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "import heapq\n",
    "import pathlib\n",
    "import shutil\n",
    "from FiD.src.model import FiDT5\n",
    "from src.model import FiDEncoderForSequenceClassification\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from src.data import BinaryCustomDatasetShuffle\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "from util import utils\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments, CustomTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7a415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))\n",
    "\n",
    "model_args, data_args, train_args = parser.parse_args_into_dataclasses([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17e3b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_architecture': 'roberta-large',\n",
       " 'model_name_or_path': 'roberta-large',\n",
       " 'config_base_path': '/data/philhoon-relevance/FiD/pretrained_models/nq_reader_large',\n",
       " 'prediction_model_name_or_path': '/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1/roberta-decisive_binary_gold_data_trial1',\n",
       " 'prediction_model_step': '380',\n",
       " 'git_tag': 'v1.1',\n",
       " 'config_name': None,\n",
       " 'tokenizer_name': None,\n",
       " 'max_seq_length': 200}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2085b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 'NQ-DEV-DPR/5-fold/1',\n",
       " 'train_file': '/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json',\n",
       " 'eval_file': '/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json',\n",
       " 'intact_eval': False,\n",
       " 'num_labels': 2,\n",
       " 'overwrite_cache': False,\n",
       " 'pad_to_max_length': False,\n",
       " 'dataset_class': 'BinaryCustomDatasetShuffle'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c23199",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = vars(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d3d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do_train': True,\n",
       " 'do_eval': True,\n",
       " 'do_predict': False,\n",
       " 'with_tracking': True,\n",
       " 'report_to': 'wandb',\n",
       " 'wandb_project': 'binary_classifier',\n",
       " 'run_name': 'testing_from_scratch',\n",
       " 'output_dir': '/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1/testing-scratch',\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'per_device_eval_batch_size': 8,\n",
       " 'seed': 42,\n",
       " 'learning_rate': 5e-05,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'lr_scheduler_type': 'linear',\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'max_train_steps': None,\n",
       " 'num_train_epochs': 3,\n",
       " 'num_warmup_steps': 0,\n",
       " 'checkpointing_steps': None,\n",
       " 'train_loss_steps': 10,\n",
       " 'save_max_limit': 3,\n",
       " 'best_metric': 'accuracy',\n",
       " 'headtype': 'adaptive',\n",
       " 'class_weights': False,\n",
       " 'num_layers': None,\n",
       " 'drop_out_rate': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf3292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92130b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646bc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611c88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeade43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f6ed75",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87912412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import T5PreTrainedModel\n",
    "import copy\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "292a9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FiD.src.model import FiDT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb71ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/philhoon-relevance/FiD/pretrained_models/nq_reader_large'\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d53d484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = FiDT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6128151",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca7ee51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"/data/philhoon-relevance/FiD/pretrained_models/nq_reader_large\",\n",
      "  \"architectures\": [\n",
      "    \"FiDT5\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6b5b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.encoder => FiDT5.EncoderWrapper\n",
    "# model.encoder.encoder => FiDT5.EncoderWrapper.encoder = T5 encoder Architecture w FiDT5 parameters\n",
    "model_encoder = model.encoder.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5200122",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained('t5-base', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3620652",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {'id': 12, \n",
    "           'question': \"when was the public service commission original version of the upsc set up\", \n",
    "           'ctx': {\"id\": \"17105334\", \n",
    "                   \"title\": \"Bihar Public Service Commission\", \n",
    "                   \"text\": \"3 of the Regulations, 1960 the Commission was constituted with a Chairman and 10 (ten) other members. The strength of members was reduced to 6 (six) after bifurcation of the State of Bihar and the State of Jharkhand vide notification no. 7/PSC-1013/95 (Part-3) Per 8262 dated 9 October 2002 of the Personnel & Administrative Reforms Department, Bihar. Article 320 and 321 of the Constitution of India prescribes the mandate of the State Public Service Commissions, which are: a)Recruitment by conduct of Competitive Examinations/ through interviews to the services of the State Government. b)Advising the State Government on the suitability of\"}\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(example['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = np.expand_dims(np.array(output['input_ids']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957fce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ids = torch.from_numpy(test_ids)\n",
    "print(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention = np.expand_dims(np.array(output['attention_mask']), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a316a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention = torch.from_numpy(test_attention)\n",
    "print(test_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_encoder.forward(input_ids = test_ids, attention_mask = test_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc662c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['last_hidden_state'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acf2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fb576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.d_model, config.d_model)\n",
    "        classifier_dropout = (\n",
    "            config.dropout_rate\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab03778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePooler(nn.Module):\n",
    "    \"\"\" Calcualte weighted average of the inputs with learnable weights \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_size = config.d_model\n",
    "        self.w = nn.Linear(self.input_size, 1, bias=True)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.classifier = nn.Linear(config.d_model, config.num_labels)\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        batch_size, seq_len, emb_dim = inputs.shape\n",
    "        scores = torch.squeeze(self.w(inputs), dim=-1)\n",
    "        weights = nn.functional.softmax(scores, dim=-1)\n",
    "        if mask is not None:\n",
    "            weights = weights * mask\n",
    "            weights = weights / weights.sum(dim=-1, keepdims=True)\n",
    "        outputs = (inputs.permute(2, 0, 1) * weights).sum(-1).T\n",
    "        \n",
    "        outputs = self.dropout(outputs)\n",
    "        logits = self.classifier(outputs)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_test(module):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.zero_()\n",
    "       \n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivePooler_.apply(init_weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79421f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivePooler_.w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f74c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.problem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiDEncoderForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config, model_encoder, pooler='adaptive'):\n",
    "        super(FiDEncoderForSequenceClassification, self).__init__()\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        \n",
    "        self.encoder = model_encoder\n",
    "        \n",
    "        classification_class = AdaptivePooler if pooler == 'adaptive' else ClassificationHead\n",
    "        self.classifier = classification_class(self.config)\n",
    "        \n",
    "        self.classifier.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=config.initializer_factor)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        logits = self.classifier(outputs[0], mask=attention_mask)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599907b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_classifier = FiDEncoderForSequenceClassification(config, model_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fedff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = encoder_classifier.forward(input_ids = test_ids, attention_mask = test_attention, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75067c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3186da",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"layer_norm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [p for n, p in encoder_classifier.named_parameters() if not any(nd in n for nd in no_decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5efb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n, p in model.named_parameters() :\n",
    "    if any(nd in n for nd in no_decay):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aef95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0208dbf",
   "metadata": {},
   "source": [
    "# Checking Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_pre = evaluate.load('precision')\n",
    "metric_re = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([1,1,1,1,1])\n",
    "references = np.array([1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e758554",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric_acc.compute()\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7013ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric_acc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c23d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4f6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19367c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdde46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245c917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "class FiDT5(transformers.T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def forward_(self, **kwargs):\n",
    "        if 'input_ids' in kwargs:\n",
    "            kwargs['input_ids'] = kwargs['input_ids'].view(kwargs['input_ids'].size(0), -1)\n",
    "        if 'attention_mask' in kwargs:\n",
    "            kwargs['attention_mask'] = kwargs['attention_mask'].view(kwargs['attention_mask'].size(0), -1)\n",
    "\n",
    "        return super(FiDT5, self).forward(\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize as B x (N * L) instead of (B * N) x L here\n",
    "    # because the T5 forward method uses the input tensors to infer\n",
    "    # dimensions used in the decoder.\n",
    "    # EncoderWrapper resizes the inputs as (B * N) x L.\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        if input_ids != None:\n",
    "            # inputs might have already be resized in the generate method\n",
    "            if input_ids.dim() == 3:\n",
    "                self.encoder.n_passages = input_ids.size(1)\n",
    "            input_ids = input_ids.view(input_ids.size(0), -1)\n",
    "        if attention_mask != None:\n",
    "            attention_mask = attention_mask.view(attention_mask.size(0), -1)\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # We need to resize the inputs here, as the generate method expect 2D tensors\n",
    "    def generate(self, input_ids, attention_mask, max_length):\n",
    "        self.encoder.n_passages = input_ids.size(1)\n",
    "        return super().generate(\n",
    "            input_ids=input_ids.view(input_ids.size(0), -1),\n",
    "            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    def wrap_encoder(self, use_checkpoint=False):\n",
    "        \"\"\"\n",
    "        Wrap T5 encoder to obtain a Fusion-in-Decoder model.\n",
    "        \"\"\"\n",
    "        self.encoder = EncoderWrapper(self.encoder, use_checkpoint=use_checkpoint)\n",
    "\n",
    "    def unwrap_encoder(self):\n",
    "        \"\"\"\n",
    "        Unwrap Fusion-in-Decoder encoder, useful to load T5 weights.\n",
    "        \"\"\"\n",
    "        self.encoder = self.encoder.encoder\n",
    "        block = []\n",
    "        for mod in self.encoder.block:\n",
    "            block.append(mod.module)\n",
    "        block = nn.ModuleList(block)\n",
    "        self.encoder.block = block\n",
    "\n",
    "    def load_t5(self, state_dict):\n",
    "        self.unwrap_encoder()\n",
    "        self.load_state_dict(state_dict)\n",
    "        self.wrap_encoder()\n",
    "\n",
    "    def set_checkpoint(self, use_checkpoint):\n",
    "        \"\"\"\n",
    "        Enable or disable checkpointing in the encoder.\n",
    "        See https://pytorch.org/docs/stable/checkpoint.html\n",
    "        \"\"\"\n",
    "        for mod in self.encoder.encoder.block:\n",
    "            mod.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def reset_score_storage(self):\n",
    "        \"\"\"\n",
    "        Reset score storage, only used when cross-attention scores are saved\n",
    "        to train a retriever.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            mod.layer[1].EncDecAttention.score_storage = None\n",
    "\n",
    "    def get_crossattention_scores(self, context_mask):\n",
    "        \"\"\"\n",
    "        Cross-attention scores are aggregated to obtain a single scalar per\n",
    "        passage. This scalar can be seen as a similarity score between the\n",
    "        question and the input passage. It is obtained by averaging the\n",
    "        cross-attention scores obtained on the first decoded token over heads,\n",
    "        layers, and tokens of the input passage.\n",
    "\n",
    "        More details in Distilling Knowledge from Reader to Retriever:\n",
    "        https://arxiv.org/abs/2012.04584.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_passages = context_mask.size(1)\n",
    "        for mod in self.decoder.block:\n",
    "            scores.append(mod.layer[1].EncDecAttention.score_storage)\n",
    "        scores = torch.cat(scores, dim=2)\n",
    "        bsz, n_heads, n_layers, _ = scores.size()\n",
    "        # batch_size, n_head, n_layers, n_passages, text_maxlength\n",
    "        scores = scores.view(bsz, n_heads, n_layers, n_passages, -1)\n",
    "        scores = scores.masked_fill(~context_mask[:, None, None], 0.)\n",
    "        scores = scores.sum(dim=[1, 2, 4])\n",
    "        ntokens = context_mask.sum(dim=[2]) * n_layers * n_heads\n",
    "        scores = scores/ntokens\n",
    "        return scores\n",
    "\n",
    "    def overwrite_forward_crossattention(self):\n",
    "        \"\"\"\n",
    "        Replace cross-attention forward function, only used to save\n",
    "        cross-attention scores.\n",
    "        \"\"\"\n",
    "        for mod in self.decoder.block:\n",
    "            attn = mod.layer[1].EncDecAttention\n",
    "            attn.forward = types.MethodType(cross_attention_forward, attn)\n",
    "\n",
    "class EncoderWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Wrapper for T5 Wrapper to obtain a Fusion-in-Decoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        apply_checkpoint_wrapper(self.encoder, use_checkpoint)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs,):\n",
    "        # total_length = n_passages * passage_length\n",
    "        bsz, total_length = input_ids.shape\n",
    "        passage_length = total_length // self.n_passages\n",
    "        input_ids = input_ids.view(bsz*self.n_passages, passage_length)\n",
    "        attention_mask = attention_mask.view(bsz*self.n_passages, passage_length)\n",
    "        outputs = self.encoder(input_ids, attention_mask, **kwargs)\n",
    "        outputs = (outputs[0].view(bsz, self.n_passages*passage_length, -1), ) + outputs[1:]\n",
    "        return outputs\n",
    "\n",
    "class CheckpointWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper replacing None outputs by empty tensors, which allows the use of\n",
    "    checkpointing.\n",
    "    \"\"\"\n",
    "    def __init__(self, module, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_bias, **kwargs):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            def custom_forward(*inputs):\n",
    "                output = self.module(*inputs, **kwargs)\n",
    "                empty = torch.tensor(\n",
    "                    [],\n",
    "                    dtype=torch.float,\n",
    "                    device=output[0].device,\n",
    "                    requires_grad=True)\n",
    "                output = tuple(x if x is not None else empty for x in output)\n",
    "                return output\n",
    "\n",
    "            output = torch.utils.checkpoint.checkpoint(\n",
    "                custom_forward,\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                position_bias\n",
    "            )\n",
    "            output = tuple(x if x.size() != 0 else None for x in output)\n",
    "        else:\n",
    "            output = self.module(hidden_states, attention_mask, position_bias, **kwargs)\n",
    "        return output\n",
    "\n",
    "def apply_checkpoint_wrapper(t5stack, use_checkpoint):\n",
    "    \"\"\"\n",
    "    Wrap each block of the encoder to enable checkpointing.\n",
    "    \"\"\"\n",
    "    block = []\n",
    "    for mod in t5stack.block:\n",
    "        wrapped_mod = CheckpointWrapper(mod, use_checkpoint)\n",
    "        block.append(wrapped_mod)\n",
    "    block = nn.ModuleList(block)\n",
    "    t5stack.block = block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae05859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194aa016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d00552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535af12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b3fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb1d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6681191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445c430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945bd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20fa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e91b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518e446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import BinaryCustomDatasetShuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7292457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "from util import utils\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    get_scheduler,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments \n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCustomDatasetShuffle(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length, shuffle = False):\n",
    "        if shuffle:\n",
    "            random.shuffle(instances)\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = 'question: ' + self.instances[idx]['question'] + \\\n",
    "                 ' title: ' + self.instances[idx]['ctx']['title'] + \\\n",
    "                 ' context : ' + self.instances[idx]['ctx']['text']\n",
    "        output = self.tokenizer(\n",
    "            input_,\n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length)\n",
    "\n",
    "        item = {key: val for key, val in output.items()}\n",
    "        # item['labels'] = torch.tensor(int(self.instances[idx]['em']))\n",
    "        item['labels'] = int(self.instances[idx]['em'])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b9ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_to = 'wandb'\n",
    "output_dir = '/data/philhoon-relevance/binary-classification/results/NQ-DEV-DPR/5-fold/1/scratch_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator(log_with=args.report_to, logging_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd47bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(logging_dir=output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'roberta-large'\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed884cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f384d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_mismatched_sizes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba87a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e89593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/data/philhoon-relevance/binary-classification/\\\n",
    "NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json'\n",
    "eval_file = '/data/philhoon-relevance/binary-classification/\\\n",
    "NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776df27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.open_json(train_file)\n",
    "eval_data = utils.open_json(eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fde6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec963609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BinaryCustomDatasetShuffle(train_data, tokenizer = tokenizer, \\\n",
    "                                           max_length = max_length, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = BinaryCustomDatasetShuffle(eval_data, tokenizer = tokenizer, \\\n",
    "                                           max_length = max_length, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47026d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e92816",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle = True,\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                              shuffle = True,\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a77035",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2a0d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters[0][\"weight_decay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba674bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps = 0\n",
    "# max_train_steps = \n",
    "num_train_epochs = 5\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81266a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8961a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d33b38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = per_device_train_batch_size * accelerator.num_processes * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c96172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b459680",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch = 0\n",
    "with_tracking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25897061",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    if with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        if with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "            \n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            \n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps }\"\n",
    "                if output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "                \n",
    "                \n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "         with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) \n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        \n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "        \n",
    "        metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "        \n",
    "        eval_metric = metric.compute()\n",
    "        logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "        \n",
    "        if args.with_tracking:\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"accuracy\" : eval_metric,\n",
    "                    \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": completed_steps,\n",
    "                },\n",
    "                step=completed_steps,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_pre = evaluate.load('precision')\n",
    "metric_re = evaluate.load('recall')\n",
    "metric_f1 = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18636963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7522dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "args = [\"--model_name_or_path\", 'allenai/longformer-large-4096', '--output_dir', './']\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=model_args.num_labels,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.train_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    train_instance = instances[data_args.dev_size:]\n",
    "    dev_instance = instances[:data_args.dev_size]\n",
    "    \n",
    "    train_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    dev_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.test_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    test_dataset = CustomDataset(instances, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"xnli\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize Trainer\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer, \n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_train else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7e951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.dataset_name = a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed523bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e978cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
