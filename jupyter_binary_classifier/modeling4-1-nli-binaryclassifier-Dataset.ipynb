{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cc73b0",
   "metadata": {},
   "source": [
    "# modeling4-nli-binaryclassifier-Dataset\n",
    "- CustomDataset Class for Binray input \n",
    "    1) q&a Fid Format\n",
    "    2) NLI Format\n",
    "- Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8532130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments \n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from util import utils\n",
    "# from transformers.models.longformer impor LongformerForSequenceClassification, LongformerSequenceClassifierOutput\n",
    "# from transformers.modeling_outputs import LongformerSequenceClassifierOutput\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55306e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "# parser.parse_args_into_dataclasses([])\n",
    "# args = [\"--foo\", \"1\", \"--baz\", \"quux\", \"--bar\", \"0.5\"]\n",
    "args = [\"--model_name_or_path\", 'roberta-large', '--output_dir', './']\n",
    "# (example,) = parser.parse_args_into_dataclasses(args, look_for_args_file=False)\n",
    "# args = parser.parse_args(args=[])\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "# print(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7a7253",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args : \n",
      " \n",
      "ModelArguments(model_architecture='roberta-large', model_name_or_path='roberta-large', git_tag='v1.1', config_name=None, tokenizer_name=None, max_seq_length=200)\n",
      "data_args : \n",
      " \n",
      "DataTrainingArguments(data='NQ-DEV-DPR/5-fold/1', train_file='/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json', eval_file='/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json', num_labels=2, overwrite_cache=False, pad_to_max_length=False)\n",
      "training_args : \n",
      " \n",
      "DataTrainingArguments(data='NQ-DEV-DPR/5-fold/1', train_file='/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json', eval_file='/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json', num_labels=2, overwrite_cache=False, pad_to_max_length=False)\n"
     ]
    }
   ],
   "source": [
    "print(f'model_args : \\n ')\n",
    "pprint(model_args)\n",
    "print(f'data_args : \\n ')\n",
    "pprint(data_args)\n",
    "print(f'training_args : \\n ')\n",
    "pprint(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f14e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "MY_DICT = {'key' : 'value'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbef502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--output_dir'], dest='output_dir', nargs=None, const=None, default='output directory', type=<class 'str'>, choices=None, help='output directory', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('-s', '--seed', type=int, default=42, help='random seed (default : 42)')\n",
    "parser.add_argument('-m', '--model', type=str, default='BaseModel', help='model type (default:BaseModel)')\n",
    "parser.add_argument('--train-dataset', type=str, default='NQ', help='train dataset (default:Natural Question)')\n",
    "parser.add_argument('--eval_dataset', type=str, default='NQ', help='eval dataset (default:Natural Question)')\n",
    "parser.add_argument('--output_dir', type=str, default='output directory', help='output directory')\n",
    "# parser.add_argument('--cache_directory', type = str, default = '', help='default transformer cahce directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad13a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939bf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4582d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce6cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d63a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# config file are updated when executing .save_pretrained('./')\n",
    "# model architecture depends on model_name\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-large', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "708f5569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST1\n",
    "# model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-large-4096')\n",
    "# model = AutoModel.from_pretrained('allenai/longformer-large-4096')\n",
    "# type(model)\n",
    "\n",
    "# TEST2\n",
    "# Custom LongformerForSequenceClassification\n",
    "# class CustomSequenceClassification(nn.Module):\n",
    "#     def __init__(self, model_name, num_labels):\n",
    "#         super(CustomSequenceClassification, self).__init__()\n",
    "#         self.num_labels = num_labels\n",
    "#         self.model_name = model_name\n",
    "        \n",
    "#         # Base Model Configuration\n",
    "#         self.model = AutoModel.from_pretrained(self.model_name, add_pooling_layer = False)\n",
    "#         self.config = self.model.config\n",
    "        \n",
    "#         # Custom Layers for Sequence Classification\n",
    "#         self.dense = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "#         self.out_proj = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "        \n",
    "#     def forward(self, \n",
    "#                 input_ids=None, \n",
    "#                 attention_mask=None, \n",
    "#                 labels=None):\n",
    "        \n",
    "#         outputs = self.model(\n",
    "#             input_ids=input_ids, \n",
    "#             attention_mask=attention_mask,\n",
    "#         )\n",
    "        \n",
    "#         # outputs[0] -> last hidden layer \n",
    "#         sequence_output = outputs[0]\n",
    "        \n",
    "#         # [batch, sequence, hidden] -> [:, 0, :] -> hidden state of <s> or [CLS] token \n",
    "#         hidden_states = hidden_states[:, 0, :]\n",
    "#         hidden_states = self.dropout(hidden_states)\n",
    "#         hidden_states = self.dense(hidden_states)\n",
    "#         hidden_states = torch.tanh(hidden_states)\n",
    "#         hidden_states = self.dropout(hidden_states)\n",
    "#         logits = self.out_proj(hidden_states)\n",
    "        \n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             # single_label_classification\n",
    "#             loss_fct = CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), lablels.view(-1))\n",
    "            \n",
    "#         return SequenceClassifierOutput(\n",
    "#             loss=loss, \n",
    "#             logits=logits, \n",
    "#             hidden_states=outputs.hidden_states,\n",
    "#             attentions=outputs.attentions,\n",
    "#             global_attentions=outputs.gloabl_attentions\n",
    "#         )\n",
    "\n",
    "# TEST3\n",
    "# class CustomSequenceClassification(AutoModelForSequenceClassification):\n",
    "#     def __init__(self, model_name, num_labels):\n",
    "#         super().__init__()\n",
    "#         self.model_name = model_name\n",
    "#         self.num_labels = num_labels\n",
    "#         self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        \n",
    "#     def forward(self, \n",
    "#                 input_ids=None, \n",
    "#                 attention_mask=None, \n",
    "#                 labels=None):\n",
    "        \n",
    "#         outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f43504",
   "metadata": {},
   "source": [
    "## nq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba7ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nq_test_file = '/data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-dev-multikilt.json'\n",
    "# nq_dpr_dev = open_json(nq_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6205e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nq_train_file = '/data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-train-multikilt.json'\n",
    "# nq_dpr_train = open_json(nq_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a9f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(nq_dpr_dev))\n",
    "# print(len(nq_dpr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b576b",
   "metadata": {},
   "source": [
    "# Binray data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bdce2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_file = '/data/philhoon-relevance/binary-classification\\\n",
    "/NQ-DEV-DPR/5-fold/1/binary_data/\\\n",
    "binary_ex_ctx100id_split_train_1.json'\n",
    "\n",
    "binary_dev_file = '/data/philhoon-relevance/binary-classification\\\n",
    "/NQ-DEV-DPR/5-fold/1/binary_data/\\\n",
    "binary_ex_ctx100id_split_dev_1.json'\n",
    "\n",
    "# binary_dev_file = 'binary_ex_ctx100id_split_dev_1.json'\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2447af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c788a56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_dev_1.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dev_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c6126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train = utils.open_json(binary_train_file)\n",
    "binary_dev = utils.open_json(binary_dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98df43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681487\n",
      "169978\n"
     ]
    }
   ],
   "source": [
    "print(len(binary_train))\n",
    "print(len(binary_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce19dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de881fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(binary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb03948c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'question': 'how many pages is invisible man by ralph ellison',\n",
       " 'ctx': {'id': '533920',\n",
       "  'title': 'Ralph Ellison',\n",
       "  'text': 'he was unsatisfied with the book. Ellison ultimately wrote more than 2,000 pages of this second novel but never finished it. Ellison died on April 16, 1994 of pancreatic cancer and was interred in a crypt at Trinity Church Cemetery in the Washington Heights neighborhood of Upper Manhattan. He was survived by his second wife, Fanny Ellison (November 27, 1911 – November 19, 2005). \"Invisible Man\" won the 1953 US National Book Award for Fiction. The award was his ticket into the American literary establishment. He eventually was admitted to the American Academy of Arts and Letters, received two President\\'s'},\n",
       " 'em': '0'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77350998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ctx': {'id': '533920',\n",
      "         'text': 'he was unsatisfied with the book. Ellison ultimately wrote '\n",
      "                 'more than 2,000 pages of this second novel but never '\n",
      "                 'finished it. Ellison died on April 16, 1994 of pancreatic '\n",
      "                 'cancer and was interred in a crypt at Trinity Church '\n",
      "                 'Cemetery in the Washington Heights neighborhood of Upper '\n",
      "                 'Manhattan. He was survived by his second wife, Fanny Ellison '\n",
      "                 '(November 27, 1911 – November 19, 2005). \"Invisible Man\" won '\n",
      "                 'the 1953 US National Book Award for Fiction. The award was '\n",
      "                 'his ticket into the American literary establishment. He '\n",
      "                 'eventually was admitted to the American Academy of Arts and '\n",
      "                 \"Letters, received two President's\",\n",
      "         'title': 'Ralph Ellison'},\n",
      " 'em': '0',\n",
      " 'id': 1,\n",
      " 'question': 'how many pages is invisible man by ralph ellison'}\n"
     ]
    }
   ],
   "source": [
    "pprint(binary_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        texts = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*texts, padding=padding, max_length=args.max_length, truncation=True)\n",
    "\n",
    "        if \"label\" in examples:\n",
    "            if label_to_id is not None:\n",
    "                # Map labels to IDs (not necessary for GLUE tasks)\n",
    "                result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "            else:\n",
    "                # In all cases, rename the column to labels because the model will expect that.\n",
    "                result[\"labels\"] = examples[\"label\"]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68474d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySentenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length, shuffle = False):\n",
    "        if shuffle:\n",
    "            random.shuffle(instances)\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text1_ = 'question: ' + self.instances[idx]['question']\n",
    "        \n",
    "        text2_ = 'title: ' + self.instances[idx]['ctx']['title'] + \\\n",
    "                    ' context : ' + self.instances[idx]['ctx']['text']\n",
    "        output = self.tokenizer(\n",
    "            text1_, text2_, \n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length)\n",
    "\n",
    "        item = {key: val for key, val in output.items()}\n",
    "        item['labels'] = int(self.instances[idx]['em'])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c581e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_testing = BinarySentenceDataset(binary_train,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33c034be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 40018, 35, 141, 171, 6052, 16, 20731, 313, 30, 910, 40812, 28041, 4060, 2, 2, 14691, 35, 10594, 21930, 5377, 4832, 37, 21, 36010, 2550, 19, 5, 1040, 4, 21930, 3284, 875, 55, 87, 132, 6, 151, 6052, 9, 42, 200, 5808, 53, 393, 1550, 24, 4, 21930, 962, 15, 587, 545, 6, 8148, 9, 30737, 636, 1668, 8, 21, 3222, 2050, 11, 10, 35867, 23, 13544, 2197, 12472, 11, 5, 663, 9754, 3757, 9, 11851, 6562, 4, 91, 21, 5601, 30, 39, 200, 1141, 6, 274, 13749, 21930, 36, 21206, 974, 6, 39711, 126, 759, 753, 6, 4013, 322, 22, 1121, 42152, 1554, 113, 351, 5, 23443, 382, 496, 5972, 3683, 13, 35320, 4, 20, 2354, 21, 39, 3682, 88, 5, 470, 17205, 7147, 4, 91, 2140, 21, 2641, 7, 5, 470, 3536, 9, 4455, 8, 23937, 6, 829, 80, 270, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_testing[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "772d12cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>question: how many pages is invisible man by ralph ellison</s></s>title: Ralph Ellison context : Ralph Ellison Ralph Waldo Ellison (March 1, 1913 – April 16, 1994) was an American novelist, literary critic, and scholar. Ellison is best known for his novel \"Invisible Man\", which won the National Book Award in 1953. He also wrote \"Shadow and Act\" (1964), a collection of political, social and critical essays, and \"Going to the Territory\" (1986). For \"The New York Times\", the best of these essays in addition to the novel put him \"among the gods of America\\'s literary Parnassus.\" A posthumous novel, \"Juneteenth\", was published after being assembled from voluminous notes he left upon his death. Ralph</s>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(train_dataset_testing[1]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "648b2686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2396490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca40b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e4eea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCustomDatasetShuffle(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length, shuffle = False):\n",
    "        if shuffle:\n",
    "            random.shuffle(instances)\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = 'question: ' + self.instances[idx]['question'] + \\\n",
    "                 ' title: ' + self.instances[idx]['ctx']['title'] + \\\n",
    "                 ' context : ' + self.instances[idx]['ctx']['text']\n",
    "        output = self.tokenizer(\n",
    "            input_,\n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length)\n",
    "\n",
    "        item = {key: val for key, val in output.items()}\n",
    "        # item['labels'] = torch.tensor(int(self.instances[idx]['em']))\n",
    "        item['labels'] = int(self.instances[idx]['em'])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e008cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BinaryCustomDatasetShuffle(binary_train,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c70eb215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.instances[0]['em']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5eb53b04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 40018, 35, 141, 171, 6052, 16, 20731, 313, 30, 910, 40812, 28041, 4060, 1270, 35, 10594, 21930, 5377, 4832, 10594, 21930, 10594, 15371, 139, 21930, 36, 11770, 112, 6, 38220, 126, 587, 545, 6, 8148, 43, 21, 41, 470, 29613, 6, 17205, 7745, 6, 8, 20875, 4, 21930, 16, 275, 684, 13, 39, 5808, 22, 1121, 42152, 1554, 1297, 61, 351, 5, 496, 5972, 3683, 11, 23443, 4, 91, 67, 875, 22, 45418, 8, 1783, 113, 36, 45629, 238, 10, 2783, 9, 559, 6, 592, 8, 2008, 27616, 6, 8, 22, 27524, 7, 5, 23463, 113, 36, 43479, 322, 286, 22, 133, 188, 469, 1513, 1297, 5, 275, 9, 209, 27616, 11, 1285, 7, 5, 5808, 342, 123, 22, 31636, 5, 25779, 9, 730, 18, 17205, 221, 4422, 2401, 687, 72, 83, 618, 18257, 1827, 5808, 6, 22, 31036, 45248, 1297, 21, 1027, 71, 145, 14525, 31, 13103, 18140, 1827, 2775, 37, 314, 2115, 39, 744, 4, 10594, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2b098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d42f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e4d8049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ctx': {'id': '533920',\n",
      "         'text': 'he was unsatisfied with the book. Ellison ultimately wrote '\n",
      "                 'more than 2,000 pages of this second novel but never '\n",
      "                 'finished it. Ellison died on April 16, 1994 of pancreatic '\n",
      "                 'cancer and was interred in a crypt at Trinity Church '\n",
      "                 'Cemetery in the Washington Heights neighborhood of Upper '\n",
      "                 'Manhattan. He was survived by his second wife, Fanny Ellison '\n",
      "                 '(November 27, 1911 – November 19, 2005). \"Invisible Man\" won '\n",
      "                 'the 1953 US National Book Award for Fiction. The award was '\n",
      "                 'his ticket into the American literary establishment. He '\n",
      "                 'eventually was admitted to the American Academy of Arts and '\n",
      "                 \"Letters, received two President's\",\n",
      "         'title': 'Ralph Ellison'},\n",
      " 'em': '0',\n",
      " 'id': 1,\n",
      " 'question': 'how many pages is invisible man by ralph ellison'}\n"
     ]
    }
   ],
   "source": [
    "pprint(binary_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ac69b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'question': 'how many pages is invisible man by ralph ellison',\n",
       " 'ctx': {'id': '533920',\n",
       "  'title': 'Ralph Ellison',\n",
       "  'text': 'he was unsatisfied with the book. Ellison ultimately wrote more than 2,000 pages of this second novel but never finished it. Ellison died on April 16, 1994 of pancreatic cancer and was interred in a crypt at Trinity Church Cemetery in the Washington Heights neighborhood of Upper Manhattan. He was survived by his second wife, Fanny Ellison (November 27, 1911 – November 19, 2005). \"Invisible Man\" won the 1953 US National Book Award for Fiction. The award was his ticket into the American literary establishment. He eventually was admitted to the American Academy of Arts and Letters, received two President\\'s'},\n",
       " 'em': '0'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84954f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = binary_train\n",
    "idx = 0\n",
    "sep_token = tokenizer.sep_token\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "058260c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'question': 'how many pages is invisible man by ralph ellison',\n",
       " 'ctx': {'id': '533920',\n",
       "  'title': 'Ralph Ellison',\n",
       "  'text': 'he was unsatisfied with the book. Ellison ultimately wrote more than 2,000 pages of this second novel but never finished it. Ellison died on April 16, 1994 of pancreatic cancer and was interred in a crypt at Trinity Church Cemetery in the Washington Heights neighborhood of Upper Manhattan. He was survived by his second wife, Fanny Ellison (November 27, 1911 – November 19, 2005). \"Invisible Man\" won the 1953 US National Book Award for Fiction. The award was his ticket into the American literary establishment. He eventually was admitted to the American Academy of Arts and Letters, received two President\\'s'},\n",
       " 'em': '0'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d50f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = 'question: ' + instances[idx]['question'] + \\\n",
    "        ' title: ' + instances[idx]['ctx']['title'] + \\\n",
    "        ' context : ' + instances[idx]['ctx']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a32d11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(\n",
    "            input_, \n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d62a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: how many pages is invisible man by ralph ellison title: Ralph Ellison context : he was unsatisfied with the book. Ellison ultimately wrote more than 2,000 pages of this second novel but never finished it. Ellison died on April 16, 1994 of pancreatic cancer and was interred in a crypt at Trinity Church Cemetery in the Washington Heights neighborhood of Upper Manhattan. He was survived by his second wife, Fanny Ellison (November 27, 1911 – November 19, 2005). \"Invisible Man\" won the 1953 US National Book Award for Fiction. The award was his ticket into the American literary establishment. He eventually was admitted to the American Academy of Arts and Letters, received two President\\'s'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f99f8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "[0, 40018, 35, 141, 171, 6052, 16, 20731, 313, 30, 910, 40812, 28041, 4060, 1270, 35, 10594, 21930, 5377, 4832, 37, 21, 36010, 2550, 19, 5, 1040, 4, 21930, 3284, 875, 55, 87, 132, 6, 151, 6052, 9, 42, 200, 5808, 53, 393, 1550, 24, 4, 21930, 962, 15, 587, 545, 6, 8148, 9, 30737, 636, 1668, 8, 21, 3222, 2050, 11, 10, 35867, 23, 13544, 2197, 12472, 11, 5, 663, 9754, 3757, 9, 11851, 6562, 4, 91, 21, 5601, 30, 39, 200, 1141, 6, 274, 13749, 21930, 36, 21206, 974, 6, 39711, 126, 759, 753, 6, 4013, 322, 22, 1121, 42152, 1554, 113, 351, 5, 23443, 382, 496, 5972, 3683, 13, 35320, 4, 20, 2354, 21, 39, 3682, 88, 5, 470, 17205, 7147, 4, 91, 2140, 21, 2641, 7, 5, 470, 3536, 9, 4455, 8, 23937, 6, 829, 80, 270, 18, 2]\n"
     ]
    }
   ],
   "source": [
    "print(len(output['input_ids']))\n",
    "print(output['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "313e1982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'question', ':', 'Ġhow', 'Ġmany', 'Ġpages', 'Ġis', 'Ġinvisible', 'Ġman', 'Ġby', 'Ġr', 'alph', 'Ġell', 'ison', 'Ġtitle', ':', 'ĠRalph', 'ĠEllison', 'Ġcontext', 'Ġ:', 'Ġhe', 'Ġwas', 'Ġunsatisf', 'ied', 'Ġwith', 'Ġthe', 'Ġbook', '.', 'ĠEllison', 'Ġultimately', 'Ġwrote', 'Ġmore', 'Ġthan', 'Ġ2', ',', '000', 'Ġpages', 'Ġof', 'Ġthis', 'Ġsecond', 'Ġnovel', 'Ġbut', 'Ġnever', 'Ġfinished', 'Ġit', '.', 'ĠEllison', 'Ġdied', 'Ġon', 'ĠApril', 'Ġ16', ',', 'Ġ1994', 'Ġof', 'Ġpancreat', 'ic', 'Ġcancer', 'Ġand', 'Ġwas', 'Ġinter', 'red', 'Ġin', 'Ġa', 'Ġcrypt', 'Ġat', 'ĠTrinity', 'ĠChurch', 'ĠCemetery', 'Ġin', 'Ġthe', 'ĠWashington', 'ĠHeights', 'Ġneighborhood', 'Ġof', 'ĠUpper', 'ĠManhattan', '.', 'ĠHe', 'Ġwas', 'Ġsurvived', 'Ġby', 'Ġhis', 'Ġsecond', 'Ġwife', ',', 'ĠF', 'anny', 'ĠEllison', 'Ġ(', 'November', 'Ġ27', ',', 'Ġ1911', 'ĠâĢĵ', 'ĠNovember', 'Ġ19', ',', 'Ġ2005', ').', 'Ġ\"', 'In', 'visible', 'ĠMan', '\"', 'Ġwon', 'Ġthe', 'Ġ1953', 'ĠUS', 'ĠNational', 'ĠBook', 'ĠAward', 'Ġfor', 'ĠFiction', '.', 'ĠThe', 'Ġaward', 'Ġwas', 'Ġhis', 'Ġticket', 'Ġinto', 'Ġthe', 'ĠAmerican', 'Ġliterary', 'Ġestablishment', '.', 'ĠHe', 'Ġeventually', 'Ġwas', 'Ġadmitted', 'Ġto', 'Ġthe', 'ĠAmerican', 'ĠAcademy', 'Ġof', 'ĠArts', 'Ġand', 'ĠLetters', ',', 'Ġreceived', 'Ġtwo', 'ĠPresident', \"'s\", '</s>']\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(output['input_ids']))\n",
    "print(len(tokenizer.convert_ids_to_tokens(output['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05b77c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>question: how many pages is invisible man by ralph ellison title: Ralph Ellison context : he was unsatisfied with the book. Ellison ultimately wrote more than 2,000 pages of this second novel but never finished it. Ellison died on April 16, 1994 of pancreatic cancer and was interred in a crypt at Trinity Church Cemetery in the Washington Heights neighborhood of Upper Manhattan. He was survived by his second wife, Fanny Ellison (November 27, 1911 – November 19, 2005). \"Invisible Man\" won the 1953 US National Book Award for Fiction. The award was his ticket into the American literary establishment. He eventually was admitted to the American Academy of Arts and Letters, received two President\\'s</s>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(output['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffa25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for idx, instance in tqdm(enumerate(instances)):\n",
    "    input_ = 'question: ' + instance['question'] + \\\n",
    "        ' title: ' + instance['ctx']['title'] + \\\n",
    "        ' context : ' + instance['ctx']['text']\n",
    "    \n",
    "    output = tokenizer(\n",
    "            input_, \n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            max_length=max_length)\n",
    "    \n",
    "    if len(output['input_ids']) > max_length:\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a15a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6294780",
   "metadata": {},
   "outputs": [],
   "source": [
    "'question: ' + binary_train[0]['question'] + ' title: ' + binary_train[0]['ctx']['title'] + ' context : ' + binary_train[0]['ctx']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train[0]['ctx']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train[0]['ctx']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de936f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "        tokenizer, \n",
    "        pad_to_multiple_of=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer implement dataloader inside\n",
    "# data_loader = DataLoader(\n",
    "#     dataset, \n",
    "#     batch_size=8, \n",
    "#     shuffle=True, \n",
    "#     sampler=None,\n",
    "#     batch_sampler=None, \n",
    "#     num_workers=1, \n",
    "#     collate_fn=data_collator,\n",
    "#     pin_memory=False, \n",
    "#     drop_last=False, \n",
    "#     timeout=0,\n",
    "#     worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85db82",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c233f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd478dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fa945",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05191f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594be105",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb38ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
