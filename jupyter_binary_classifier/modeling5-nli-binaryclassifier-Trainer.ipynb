{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e92895c",
   "metadata": {},
   "source": [
    "# modeling5-nli-binaryclassifier-Trainer\n",
    "- nli-binaryclassifier using Huggingface Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7292457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments \n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6321cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(file):\n",
    "    with open(file , 'r') as f: \n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb0eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = '/data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-train-multikilt.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4d201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open_json(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0365752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(json_file, sample_size:int, position:int):\n",
    "    \"\"\"\n",
    "    sample_size : one to five\n",
    "        e.g.)\n",
    "            positive_sample = 1 positive passage + n-1 negative passage\n",
    "            negative_sample = n negative passage\n",
    "    cut_off : number of questions discarded when there is not enough negative passages\n",
    "    position : position of positive passage (1 ~ n)\n",
    "        e.g.) n = 2, position = 1\n",
    "            instance = [negative passage, positive passage]\n",
    "    \"\"\"\n",
    "    cut_off = 0\n",
    "    instances = []\n",
    "    sample_size = sample_size\n",
    "    position = position\n",
    "    total_questions = len(json_file) \n",
    "    \n",
    "    for idx, samples in enumerate(json_file):\n",
    "        answer = samples['answers'] \n",
    "        question = samples['question']\n",
    "        negative_samples = []\n",
    "    \n",
    "        # 'hard_negative_ctxs' should be at least equal to sample_size\n",
    "        # 'positive_ctx' which contains the answer should be at least one\n",
    "        if len(samples['hard_negative_ctxs']) < sample_size or len(samples['positive_ctxs']) < 1:\n",
    "            cut_off += 1\n",
    "        else:\n",
    "            cnt_negative_sample = 0\n",
    "            for negative_sample in samples['hard_negative_ctxs']:\n",
    "                if cnt_negative_sample > sample_size - 1:\n",
    "                    break\n",
    "                ng_s = negative_sample['text'].replace('\\n', ' ')\n",
    "                negative_samples.append(ng_s)\n",
    "                cnt_negative_sample += 1\n",
    "            \n",
    "            # 'hard_negative_ctxs' sorted by its score, so shuffle them\n",
    "            random.shuffle(negative_samples)\n",
    "            \n",
    "            # replace 1 negative_sample with one positive_sample in designated position\n",
    "            positive_sample = samples['positive_ctxs'][0]['text'].replace('\\n', ' ')\n",
    "            positive_samples = deepcopy(negative_samples)\n",
    "            positive_samples[position-1] = positive_sample \n",
    "            \n",
    "            negative_template={\n",
    "            'text' : negative_samples,\n",
    "            'labels' : 0,\n",
    "            'answer' : answer,\n",
    "            'question' : question,\n",
    "            }\n",
    "            positive_template={\n",
    "                'text' : positive_samples,\n",
    "                'labels' : 1,\n",
    "                'answer' : answer,\n",
    "                'question' : question,\n",
    "                'pos' : position,\n",
    "            }\n",
    "            instances.append(negative_template)\n",
    "            instances.append(positive_template)\n",
    "    \n",
    "    return instances, cut_off, total_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64215b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances, cut_off, total_questions = preprocessing_data(\n",
    "            train_data,\n",
    "            5,\n",
    "            5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9daf56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66cb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ['Tracy McConnell'],\n",
      " 'labels': 0,\n",
      " 'question': 'how i.met your mother who is the mother',\n",
      " 'text': ['Hopeless (How I Met Your Mother) \"Hopeless\" is the 21st episode of '\n",
      "          'the sixth season of the CBS sitcom \"How I Met Your Mother\" and the '\n",
      "          '133rd episode overall. It aired on April 18, 2011. Plot. The '\n",
      "          'episode starts with Barney and his father, Jerry, parting in 1983. '\n",
      "          \"Back in the present, Barney is disappointed in Jerry's normal \"\n",
      "          'suburban lifestyle, and decides not to pursue any further contact. '\n",
      "          'However, Barney is surprised by a call from Jerry, who',\n",
      "          'Craig Thomas (screenwriter) Craig David Thomas is an American '\n",
      "          'television writer who, along with writing partner Carter Bays, has '\n",
      "          'written episodes of \"American Dad!\", \"Oliver Beene\", \"Quintuplets\" '\n",
      "          'and the hit CBS sitcom \"How I Met Your Mother\", which they created '\n",
      "          'in 2005. In 2012 \"How I Met Your Mother\" won Best Comedy at the '\n",
      "          \"People's Choice Awards. Along with Carter Bays he is a member of \"\n",
      "          'The Solids, who perform the theme song to \"How',\n",
      "          'Rally (How I Met Your Mother) \"Rally\" is the eighteenth episode of '\n",
      "          'the ninth season of the CBS sitcom \"How I Met Your Mother\", and the '\n",
      "          '202nd episode overall. Plot. In the future, Ted and The Mother are '\n",
      "          \"in the back of a limo on their way to a New Year's Eve party. Ted \"\n",
      "          'pops open some champagne to toast to a great year with his wife, '\n",
      "          'who has just released a new book. The Mother warns Ted to go easy '\n",
      "          'on the',\n",
      "          'Pilot (How I Met Your Mother) \"Pilot\" is the pilot episode of '\n",
      "          'American television sitcom \"How I Met Your Mother\", which premiered '\n",
      "          'on CBS on September 19, 2005. It was written by series creators '\n",
      "          'Carter Bays and Craig Thomas, and directed by Pamela Fryman The '\n",
      "          'pilot takes place in 2030, as a future Ted Mosby (Voiced by Bob '\n",
      "          'Saget) is telling his kids the story of how he met their mother. It '\n",
      "          'flashes back to 2005 to a younger Ted (Josh Radnor) who',\n",
      "          'Happily Ever After (How I Met Your Mother) \"Happily Ever After\" is '\n",
      "          'the sixth episode in the fourth season of the television series '\n",
      "          '\"How I Met Your Mother\" and 70th overall. It originally aired on '\n",
      "          'November 3, 2008. Plot. The day after Stella ran out on their '\n",
      "          'wedding, Ted is unusually happy. His friends (other than Barney, '\n",
      "          \"who is just happy Ted isn't married) are worried about him, \"\n",
      "          'thinking that he is repressing his emotions. Over the course of']}\n"
     ]
    }
   ],
   "source": [
    "pprint(instances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "268e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_file = '/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1.json'\n",
    "binary_dev_file = 'binary_ex_ctx100id_split_dev_1.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f584a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_data = open_json(binary_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b0e83ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ctx': {'id': '533920',\n",
      "         'text': 'he was unsatisfied with the book. Ellison ultimately wrote '\n",
      "                 'more than 2,000 pages of this second novel but never '\n",
      "                 'finished it. Ellison died on April 16, 1994 of pancreatic '\n",
      "                 'cancer and was interred in a crypt at Trinity Church '\n",
      "                 'Cemetery in the Washington Heights neighborhood of Upper '\n",
      "                 'Manhattan. He was survived by his second wife, Fanny Ellison '\n",
      "                 '(November 27, 1911 – November 19, 2005). \"Invisible Man\" won '\n",
      "                 'the 1953 US National Book Award for Fiction. The award was '\n",
      "                 'his ticket into the American literary establishment. He '\n",
      "                 'eventually was admitted to the American Academy of Arts and '\n",
      "                 \"Letters, received two President's\",\n",
      "         'title': 'Ralph Ellison'},\n",
      " 'em': '0',\n",
      " 'id': 1,\n",
      " 'question': 'how many pages is invisible man by ralph ellison'}\n"
     ]
    }
   ],
   "source": [
    "pprint(binary_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034ad98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300f128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55525e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa92fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length):\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ = [' ' + self.instances[idx]['question']] + self.instances[idx]['text']\n",
    "        input_txt =  f' { self.sep_token } '.join(input_) + ' '\n",
    "        \n",
    "        output = self.tokenizer(\n",
    "            input_txt, \n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length)\n",
    "        \n",
    "        item = {key : val for key, val in output.items()}\n",
    "        item['labels'] = torch.tensor(self.instances[idx]['labels'])\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b3d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "args = [\"--model_name_or_path\", 'allenai/longformer-large-4096', '--output_dir', './']\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cab07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/21/2022 17:17:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 3distributed training: False, 16-bits training: False\n",
      "12/21/2022 17:17:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=3,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./runs/Dec21_17-17-06_desktop1.xfact.net,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e57646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2ebdca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelArguments' object has no attribute 'num_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m         model_args\u001b[38;5;241m.\u001b[39mconfig_name \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mconfig_name \u001b[38;5;28;01melse\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[0;32m----> 3\u001b[0m         num_labels\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m,\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m         model_args\u001b[38;5;241m.\u001b[39mtokenizer_name \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mtokenizer_name \u001b[38;5;28;01melse\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m     10\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelArguments' object has no attribute 'num_labels'"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=model_args.num_labels,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df813504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.train_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    train_instance = instances[data_args.dev_size:]\n",
    "    dev_instance = instances[:data_args.dev_size]\n",
    "    \n",
    "    train_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    dev_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.test_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    test_dataset = CustomDataset(instances, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7794c379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a7eafab12d430ba85dcc88c8f74734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"xnli\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize Trainer\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer, \n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_train else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7e951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0b9adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"xnli\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Computes XNLI score which is just simple accuracy.\n",
       "Args:\n",
       "    predictions: Predicted labels.\n",
       "    references: Ground truth labels.\n",
       "Returns:\n",
       "    'accuracy': accuracy\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> references = [0, 1]\n",
       "    >>> xnli_metric = evaluate.load(\"xnli\")\n",
       "    >>> results = xnli_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8df12bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(dataset_name='../data/train_dataset', overwrite_cache=False, max_seq_length=1024, pad_to_max_length=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args.dataset_name = a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed523bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e978cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allenai/longformer-large-4096'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
