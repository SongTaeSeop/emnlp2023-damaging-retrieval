{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fdf4e8",
   "metadata": {},
   "source": [
    "# modeling1-Longformer-datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8532130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments \n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "# from transformers.models.longformer impor LongformerForSequenceClassification, LongformerSequenceClassifierOutput\n",
    "# from transformers.modeling_outputs import LongformerSequenceClassifierOutput\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7df4737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06f8644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='roberta-large', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21ae506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        max_length=200,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51ec7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xnli (/home/philhoon/.cache/huggingface/datasets/xnli/de/1.1.0/818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\n",
    "                \"xnli\",\n",
    "                'de',\n",
    "                split=\"validation\",\n",
    "                use_auth_token=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ddc348b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1b0e87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3060310d68d402f93bcf2749303d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16c61efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71135f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87053b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'und er hat gesagt, Mama ich bin daheim.',\n",
       " 'hypothesis': 'Er rief seine Mutter an, sobald er aus dem Schulbus stieg.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55306e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = HfArgumentParser(\n",
    "#         (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "#     )\n",
    "# # parser.parse_args_into_dataclasses([])\n",
    "# # args = [\"--foo\", \"1\", \"--baz\", \"quux\", \"--bar\", \"0.5\"]\n",
    "# args = [\"--model_name_or_path\", 'allenai/longformer-large-4096', '--output_dir', './']\n",
    "# # (example,) = parser.parse_args_into_dataclasses(args, look_for_args_file=False)\n",
    "# # args = parser.parse_args(args=[])\n",
    "# model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n",
    "# # print(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a7a7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args\n",
    "# data_args\n",
    "# training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f14e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "MY_DICT = {'key' : 'value'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bbef502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--output_dir'], dest='output_dir', nargs=None, const=None, default='output directory', type=<class 'str'>, choices=None, help='output directory', metavar=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('-s', '--seed', type=int, default=42, help='random seed (default : 42)')\n",
    "parser.add_argument('-m', '--model', type=str, default='BaseModel', help='model type (default:BaseModel)')\n",
    "parser.add_argument('--train-dataset', type=str, default='NQ', help='train dataset (default:Natural Question)')\n",
    "parser.add_argument('--eval_dataset', type=str, default='NQ', help='eval dataset (default:Natural Question)')\n",
    "parser.add_argument('--output_dir', type=str, default='output directory', help='output directory')\n",
    "# parser.add_argument('--cache_directory', type = str, default = '', help='default transformer cahce directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fad13a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "939bf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4582d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ce6cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d63a809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa94d7c3bb47485d8b1505a2ba96a888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3821d14b2d584ab3af50ac51ddbf363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# config file are updated when executing .save_pretrained('./')\n",
    "# model architecture depends on model_name\n",
    "model = AutoModelForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708f5569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST1\n",
    "# model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-large-4096')\n",
    "# model = AutoModel.from_pretrained('allenai/longformer-large-4096')\n",
    "# type(model)\n",
    "\n",
    "# TEST2\n",
    "# Custom LongformerForSequenceClassification\n",
    "# class CustomSequenceClassification(nn.Module):\n",
    "#     def __init__(self, model_name, num_labels):\n",
    "#         super(CustomSequenceClassification, self).__init__()\n",
    "#         self.num_labels = num_labels\n",
    "#         self.model_name = model_name\n",
    "        \n",
    "#         # Base Model Configuration\n",
    "#         self.model = AutoModel.from_pretrained(self.model_name, add_pooling_layer = False)\n",
    "#         self.config = self.model.config\n",
    "        \n",
    "#         # Custom Layers for Sequence Classification\n",
    "#         self.dense = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "#         self.out_proj = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "        \n",
    "#     def forward(self, \n",
    "#                 input_ids=None, \n",
    "#                 attention_mask=None, \n",
    "#                 labels=None):\n",
    "        \n",
    "#         outputs = self.model(\n",
    "#             input_ids=input_ids, \n",
    "#             attention_mask=attention_mask,\n",
    "#         )\n",
    "        \n",
    "#         # outputs[0] -> last hidden layer \n",
    "#         sequence_output = outputs[0]\n",
    "        \n",
    "#         # [batch, sequence, hidden] -> [:, 0, :] -> hidden state of <s> or [CLS] token \n",
    "#         hidden_states = hidden_states[:, 0, :]\n",
    "#         hidden_states = self.dropout(hidden_states)\n",
    "#         hidden_states = self.dense(hidden_states)\n",
    "#         hidden_states = torch.tanh(hidden_states)\n",
    "#         hidden_states = self.dropout(hidden_states)\n",
    "#         logits = self.out_proj(hidden_states)\n",
    "        \n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             # single_label_classification\n",
    "#             loss_fct = CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), lablels.view(-1))\n",
    "            \n",
    "#         return SequenceClassifierOutput(\n",
    "#             loss=loss, \n",
    "#             logits=logits, \n",
    "#             hidden_states=outputs.hidden_states,\n",
    "#             attentions=outputs.attentions,\n",
    "#             global_attentions=outputs.gloabl_attentions\n",
    "#         )\n",
    "\n",
    "# TEST3\n",
    "# class CustomSequenceClassification(AutoModelForSequenceClassification):\n",
    "#     def __init__(self, model_name, num_labels):\n",
    "#         super().__init__()\n",
    "#         self.model_name = model_name\n",
    "#         self.num_labels = num_labels\n",
    "#         self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        \n",
    "#     def forward(self, \n",
    "#                 input_ids=None, \n",
    "#                 attention_mask=None, \n",
    "#                 labels=None):\n",
    "        \n",
    "#         outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f43504",
   "metadata": {},
   "source": [
    "## nq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e08ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(file):\n",
    "    with open(file , 'r') as f: \n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ba7ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_test_file = '/data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-dev-multikilt.json'\n",
    "nq_dpr_dev = open_json(nq_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6205e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train_file = '/data/philhoon-relevance/KILT/kilt-dpr-retrieval/nq-train-multikilt.json'\n",
    "nq_dpr_train = open_json(nq_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00a9f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2837\n",
      "76945\n"
     ]
    }
   ],
   "source": [
    "print(len(nq_dpr_dev))\n",
    "print(len(nq_dpr_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe67e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pprint(nq_dpr_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce19dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'allenai/longformer-large-4096'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec992421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(nq_dpr_train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dedd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cut_off = 0\n",
    "# # negative_sample_size = 5\n",
    "# # position = 0\n",
    "# total_questions = len(nq_dpr_train)\n",
    "# cnt = 0 \n",
    "# for samples in nq_dpr_train:\n",
    "# #     negatives = []\n",
    "# #     pprint(samples)\n",
    "# #     question = samples['question']\n",
    "# #     answer = answer['question']\n",
    "# #     negative_samples = []  \n",
    "\n",
    "# #     positive_sample = []\n",
    "# #     if len(samples['positive_ctxs']) < 1:\n",
    "# # #         pprint(samples)\n",
    "# # #         break\n",
    "# #         cnt += 1\n",
    "# #     if len(samples['hard_negative_ctxs']) < 5:\n",
    "# # #         pprint(samples)\n",
    "# # #         break\n",
    "# #         cnt += 1\n",
    "#     if len(samples['hard_negative_ctxs']) < 5 or len(samples['positive_ctxs']) < 1:\n",
    "#         cnt += 1\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# mylist = [\"apple\", \"banana\", \"cherry\"]\n",
    "# random.shuffle(mylist)\n",
    "\n",
    "# print(mylist)\n",
    "# new_lst = deepcopy(mylist)\n",
    "# new_lst[1] = 'pos'\n",
    "# print(new_lst)\n",
    "# print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a03805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_off = 0\n",
    "# instances = []\n",
    "# sample_size = 4\n",
    "# position = 2\n",
    "\n",
    "# total_quetions = len(nq_dpr_train)\n",
    "# for samples in nq_dpr_train:\n",
    "# #     negatives = []\n",
    "# #     pprint(samples)\n",
    "# #     question = samples['question']\n",
    "#     answer = samples['answers'] \n",
    "#     question = samples['question']\n",
    "#     negative_samples = []\n",
    "#     # sample_size = 4\n",
    "#     # position 2\n",
    "#     # random.shuffle(negative_samples)\n",
    "#     # negative_samples = [n3, n1, n2, n4]\n",
    "#     # adding positive context at position 2\n",
    "#     # positive_samples = [n3, n1, p1, n3] \n",
    "    \n",
    "#     # 'hard_negative_ctxs' should be at least equal to sample_size\n",
    "#     if len(samples['hard_negative_ctxs']) < sample_size or len(samples['positive_ctxs']) < 1:\n",
    "#         cut_off += 1\n",
    "#     else:\n",
    "#         cnt_negative_sample = 0\n",
    "#         for negative_sample in samples['hard_negative_ctxs']:\n",
    "#             if cnt_negative_sample > sample_size - 1:\n",
    "#                 break\n",
    "#             ng_s = negative_sample['text'].replace('\\n', ' ')\n",
    "#             negative_samples.append(ng_s)\n",
    "#             cnt_negative_sample += 1\n",
    "            \n",
    "#         random.shuffle(negative_samples)\n",
    "        \n",
    "#         # replace 1 negative_sample with positive_sample\n",
    "#         positive_sample = samples['positive_ctxs'][0]['text'].replace('\\n', ' ')\n",
    "#         positive_samples = deepcopy(negative_samples)\n",
    "#         positive_samples[position] = positive_sample\n",
    "        \n",
    "#         negative_template={\n",
    "#             'text' : negative_samples,\n",
    "#             'labels' : 0,\n",
    "#             'answer' : answer,\n",
    "#             'question' : question,\n",
    "#         }\n",
    "#         positive_template={\n",
    "#             'text' : positive_samples,\n",
    "#             'labels' : 1,\n",
    "#             'answer' : answer,\n",
    "#             'question' : question,\n",
    "#             'pos' : position,\n",
    "#         }\n",
    "#         instances.append(negative_template)\n",
    "#         instances.append(positive_template)\n",
    "\n",
    "# print(cut_off)\n",
    "\n",
    "    \n",
    "# #         pprint(samples['hard_negative_ctxs'])\n",
    "# #         pprint(samples['psg_id'])\n",
    "# #     pprint(samples['positive_ctx'])\n",
    "# # print(cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401bd218",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# sample_size = 5\n",
    "\n",
    "# for instance in nq_dpr_train:\n",
    "# #     print(len(instance['hard_negative_ctxs']) >= 5)\n",
    "#     if len(instance['hard_negative_ctxs']) < sample_size:\n",
    "#         cnt += 1\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02068bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(json_file, sample_size:int, position:int):\n",
    "    \"\"\"\n",
    "    sample_size : one to five\n",
    "        e.g.)\n",
    "            positive_sample = 1 positive passage + n-1 negative passage\n",
    "            negative_sample = n negative passage\n",
    "    cut_off : number of questions discarded when there is not enough negative passages\n",
    "    position : position of positive passage (1 ~ n)\n",
    "        e.g.) n = 2, position = 1\n",
    "            instance = [negative passage, positive passage]\n",
    "    \"\"\"\n",
    "    cut_off = 0\n",
    "    instances = []\n",
    "    sample_size = sample_size\n",
    "    position = position\n",
    "    total_questions = len(json_file) \n",
    "    \n",
    "    for idx, samples in enumerate(json_file):\n",
    "        answer = samples['answers'] \n",
    "        question = samples['question']\n",
    "        negative_samples = []\n",
    "    \n",
    "        # 'hard_negative_ctxs' should be at least equal to sample_size\n",
    "        # 'positive_ctx' which contains the answer should be at least one\n",
    "        if len(samples['hard_negative_ctxs']) < sample_size or len(samples['positive_ctxs']) < 1:\n",
    "            cut_off += 1\n",
    "        else:\n",
    "            cnt_negative_sample = 0\n",
    "            for negative_sample in samples['hard_negative_ctxs']:\n",
    "                if cnt_negative_sample > sample_size - 1:\n",
    "                    break\n",
    "                ng_s = negative_sample['text'].replace('\\n', ' ')\n",
    "                negative_samples.append(ng_s)\n",
    "                cnt_negative_sample += 1\n",
    "            \n",
    "            # 'hard_negative_ctxs' sorted by its score, so shuffle them\n",
    "            random.shuffle(negative_samples)\n",
    "            \n",
    "            # replace 1 negative_sample with one positive_sample in designated position\n",
    "            positive_sample = samples['positive_ctxs'][0]['text'].replace('\\n', ' ')\n",
    "            positive_samples = deepcopy(negative_samples)\n",
    "            positive_samples[position-1] = positive_sample \n",
    "            \n",
    "            negative_template={\n",
    "            'text' : negative_samples,\n",
    "            'labels' : 0,\n",
    "            'answer' : answer,\n",
    "            'question' : question,\n",
    "            }\n",
    "            positive_template={\n",
    "                'text' : positive_samples,\n",
    "                'labels' : 1,\n",
    "                'answer' : answer,\n",
    "                'question' : question,\n",
    "                'pos' : position,\n",
    "            }\n",
    "            instances.append(negative_template)\n",
    "            instances.append(positive_template)\n",
    "    \n",
    "    return instances, cut_off, total_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fce5405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pprint(nq_dpr_dev[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49974848",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_size = 5\n",
    "position = 1\n",
    "instances, cut_off, total_questions = preprocessing_data(nq_dpr_train, sample_size, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acae5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76945\n",
      "2801\n"
     ]
    }
   ],
   "source": [
    "print(total_questions)\n",
    "print(cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2156a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148288\n"
     ]
    }
   ],
   "source": [
    "print(len(instances))\n",
    "dev_instance = instances[:3000]\n",
    "train_instance = instances[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8108cb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "145288\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_instance))\n",
    "print(len(train_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60946263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['4:38 2. \"Lover Man (Oh Where Can You Be?)\" (Davis, Ramirez, Sherman) – 5:25 3. \"Ah, That\\'s Freedom\" – 10:08 4. \"Don\\'t Ever Leave Me\" – 4:28 5. \"Willow Weep for Me\" (Ronell) – 6:15 6. \"Mean What You Say\" – 5:51 7. \"Once Around\" – 12:45 8. \"Polka Dots and Moonbeams\" (Van Heusen, Burke) – 4:02',\n",
       "  ', 3(2):2200, 2008. - Misha Gromov. \"Topological invariants of dynamical systems and spaces of holomorphic maps\". I. Math. Phys. Anal. Geom., 2(4):323–415, 1999. - Elon Lindenstrauss and Benjamin Weiss. \"Mean topological dimension\". Israel J. Math., 115:1–24, 2000. External links. What is Mean Dimension?',\n",
       "  '1. \"Introduction (Mel Lewis & Alan Grant)\" – 1:50 2. \"Big Dipper\" – 5:10 3. \"Polka Dots and Moonbeams\" (Van Heusen, Burke) – 3:47 4. \"Once Around\" – 12:37 5. \"All My Yesterdays\" – 4:08 6. \"Morning Reverend\" – 4:50 7. \"Low Down\" – 4:25 8. \"Lover Man\" (Davis, Ramirez, Sherman) – 5:08 9. \"Mean What',\n",
       "  'Track listings appear in a slightly different order on the reissued version as well. Singles from this album include \"My Girl\", and \"Do You Know What I Mean\" (with background vocals provided by Lee Aaron). Track listing. Track listing Original release. 1. \"Veil of Tears\" – 4:23 2. \"Do You Know What I Mean\" – 3:45 3. \"Caviar\" – 4:34 4. \"Sonya\" – 5:10 5. \"Head On\"',\n",
       "  'difficult, having these pre-assessments done will help you spend more time teaching students what they don\\'t know and just refreshing them on what they do already do know. For example, if you are going to be starting a new unit in math, how to add and subtract. Just by asking the students \"What does addition mean?\", \"What does subtraction mean\" and, \"Do they relate to each other?\", the teacher would be able to know that the students had a good basic'],\n",
       " 'labels': 0,\n",
       " 'answer': ['the therefore sign',\n",
       "  'therefore sign',\n",
       "  'the therefore sign ( ∴ ) is generally used before a logical consequence , such as the conclusion of a syllogism',\n",
       "  'a logical consequence , such as the conclusion of a syllogism'],\n",
       " 'question': 'what do the 3 dots mean in math'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42900d63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def checking_instance(instances, sample_size, position):\n",
    "#     for instance in instances:\n",
    "#         if len(instance['text']) != sample_size:\n",
    "#             print('not right sample_size')\n",
    "            \n",
    "#         label = instance['labels']\n",
    "#         answer_lst = instance['answer']\n",
    "        \n",
    "#         if not label:\n",
    "#             for answer in answer_lst:\n",
    "#                 for text in instance['text']:\n",
    "#                        if answer in text:\n",
    "#                             print('-------')\n",
    "#                             print('answer in negative context')\n",
    "#                             print(f'answer : {answer}')\n",
    "#                             print(f'instance ')\n",
    "#                             pprint(instance)\n",
    "#                             print('-------')\n",
    "#         else:\n",
    "#             for answer in answer_lst:\n",
    "#                 for idx, text in enumerate(instance['text'], 1):\n",
    "#                     if answer in text and idx != position:\n",
    "#                         print('-------')\n",
    "#                         print(f'answer in wrong position context')\n",
    "#                         print(f'answer : {answer}')\n",
    "#                         print(f'idx : {idx}')\n",
    "#                         print(f'position : {position}')\n",
    "#                         print(f'instance ')\n",
    "#                         pprint(instance)\n",
    "#                         print('-------')\n",
    "# checking_instance(instances, sample_size, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances_text = deepcopy(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4330f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset_2(torch.utils.data.Dataset):\n",
    "#     def __init__(self, instances, cls_token, sep_token):\n",
    "#         self.instances = instances\n",
    "#         self.cls_token = cls_token\n",
    "#         self.sep_token = sep_token\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.instances)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ = [f'{self.cls_token} ' + self.instances[idx]['question']] + self.instances[idx]['text']\n",
    "#         input_txt =  f' {self.sep_token} '.join(input_) + f' {self.sep_token}'\n",
    "#         label = self.instances[idx]['labels']\n",
    "        \n",
    "#         return {'text' : input_txt, 'label' : label}\n",
    "# dataset2 = CustomDataset_2(instances, tokenizer.cls_token, tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826782c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking tokenizer length\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# check_token_size = []\n",
    "# for i in tqdm(range(0, len(dataset2))):\n",
    "#     token_lenght = len(tokenizer(dataset2[i]['text'])['input_ids'])\n",
    "# #     token_length = dataset[i]['text'].size()[1]\n",
    "#     check_token_size.append(token_lenght)\n",
    "# print(f'percentage under 1024 {sum((i <= 1024 for i in check_token_size))/len(instances)}')\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame (check_token_size, columns = ['token_length'])\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f865c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length):\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ = [' ' + self.instances[idx]['question']] + self.instances[idx]['text']\n",
    "        input_txt =  f' { self.sep_token } '.join(input_) + ' '\n",
    "        \n",
    "        output = self.tokenizer(\n",
    "            input_txt, \n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length)\n",
    "        \n",
    "        item = {key : val for key, val in output.items()}\n",
    "        item['labels'] = torch.tensor(self.instances[idx]['labels'])\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20e340c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(dev_instance, tokenizer, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77d88112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8022342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ab7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(dataset[3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72824f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(dataset[3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a356b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# check_token_size = []\n",
    "# for i in tqdm(range(0, len(dataset))):\n",
    "#     token_length = dataset[i]['input_ids'].size()[1]\n",
    "#     check_token_size.append(token_length)\n",
    "# print(f'percentage under 1024 {sum((i <= 1024 for i in check_token_size))/len(instances)}')\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame (check_token_size, columns = ['token_length'])\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abecd57e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset[1]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a4e7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9de936f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "        tokenizer, \n",
    "        pad_to_multiple_of=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer implement dataloader inside\n",
    "# data_loader = DataLoader(\n",
    "#     dataset, \n",
    "#     batch_size=8, \n",
    "#     shuffle=True, \n",
    "#     sampler=None,\n",
    "#     batch_sampler=None, \n",
    "#     num_workers=1, \n",
    "#     collate_fn=data_collator,\n",
    "#     pin_memory=False, \n",
    "#     drop_last=False, \n",
    "#     timeout=0,\n",
    "#     worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d865fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15befb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309c05b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in k['input_ids'].numpy():\n",
    "#     print(tokenizer.convert_ids_to_tokens(i))\n",
    "#     print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f26305",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in data_loader:\n",
    "#     print(i['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eabdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab3775",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "#            batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "#            pin_memory=False, drop_last=False, timeout=0,\n",
    "#            worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ff2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c590b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45ecaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3178e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58f73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f8357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# def gen():\n",
    "#     for idx in len(dataset):\n",
    "#         yield dataset[idx]  # this has to be a dictionary\n",
    "#     ## or if it's an IterableDataset\n",
    "#     # for ex in torch_dataset:\n",
    "#     #     yield ex\n",
    "\n",
    "# dset = Dataset.from_generator(gen)\n",
    "\n",
    "# on-the-fly transformation\n",
    "# class Encoder():\n",
    "#     def __init__(tokenizer):\n",
    "#         self.tokenizer = tokenizer\n",
    "        \n",
    "#     def encode(examples):\n",
    "#         return self.tokenizer(\n",
    "#             examples['text'],\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=False,\n",
    "#             max_length = max_length\n",
    "#         )\n",
    "\n",
    "# def encode(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['text'],\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         add_special_tokens=False,\n",
    "#         max_length = max_length\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ad4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
