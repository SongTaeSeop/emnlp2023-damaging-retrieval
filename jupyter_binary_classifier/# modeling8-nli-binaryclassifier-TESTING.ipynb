{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e92895c",
   "metadata": {},
   "source": [
    "# modeling8-nli-binaryclassifier-TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7292457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import evaluate\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModel, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from util.arguments import ModelArguments, DataTrainingArguments \n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from pprint import pprint\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6321cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(file):\n",
    "    with open(file , 'r') as f: \n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_file = '/data/philhoon-relevance/binary-classification/NQ-DEV-DPR/5-fold/1/binary_data/binary_ex_ctx100id_split_train_1_partial.json'\n",
    "# binary_dev_file = 'binary_ex_ctx100id_split_dev_1.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f584a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_data = open_json(binary_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0e83ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ctx': {'id': '533920',\n",
      "         'text': 'he was unsatisfied with the book. Ellison ultimately wrote '\n",
      "                 'more than 2,000 pages of this second novel but never '\n",
      "                 'finished it. Ellison died on April 16, 1994 of pancreatic '\n",
      "                 'cancer and was interred in a crypt at Trinity Church '\n",
      "                 'Cemetery in the Washington Heights neighborhood of Upper '\n",
      "                 'Manhattan. He was survived by his second wife, Fanny Ellison '\n",
      "                 '(November 27, 1911 â€“ November 19, 2005). \"Invisible Man\" won '\n",
      "                 'the 1953 US National Book Award for Fiction. The award was '\n",
      "                 'his ticket into the American literary establishment. He '\n",
      "                 'eventually was admitted to the American Academy of Arts and '\n",
      "                 \"Letters, received two President's\",\n",
      "         'title': 'Ralph Ellison'},\n",
      " 'em': '0',\n",
      " 'id': 1,\n",
      " 'question': 'how many pages is invisible man by ralph ellison'}\n"
     ]
    }
   ],
   "source": [
    "pprint(binary_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f034ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length):\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = 'question: ' + self.instances[idx]['question'] + \\\n",
    "                 ' title: ' + self.instances[idx]['ctx']['title'] + \\\n",
    "                 ' context : ' + self.instances[idx]['ctx']['text']\n",
    "        output = self.tokenizer(\n",
    "            input_,\n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length)\n",
    "\n",
    "        item = {key: val for key, val in output.items()}\n",
    "        # item['labels'] = torch.tensor(int(self.instances[idx]['em']))\n",
    "        item['labels'] = int(self.instances[idx]['em'])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e8418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = None\n",
    "model_name_or_path = 'roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ac12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(\n",
    "#         model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "#         num_labels=data_args.num_labels,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7c02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        config_name if config_name else model_name_or_path,\n",
    "        num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e1b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "pprint((config.num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a977bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name if tokenizer_name else model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3416f16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='roberta-large', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfad8951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb08e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1249d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d5b7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2514bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = BinaryCustomDataset(binary_train_data,\n",
    "#                                       tokenizer,\n",
    "#                                       max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f212b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCustomDatasetShuffle(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length, shuffle = False):\n",
    "        if shuffle:\n",
    "            random.shuffle(instances)\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _shuffle(self, instances):\n",
    "        return random.shuffle(instances)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ = 'question: ' + self.instances[idx]['question'] + \\\n",
    "                 ' title: ' + self.instances[idx]['ctx']['title'] + \\\n",
    "                 ' context : ' + self.instances[idx]['ctx']['text']\n",
    "        output = self.tokenizer(\n",
    "            input_,\n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length)\n",
    "\n",
    "        item = {key: val for key, val in output.items()}\n",
    "        # item['labels'] = torch.tensor(int(self.instances[idx]['em']))\n",
    "        item['labels'] = int(self.instances[idx]['em'])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48021938",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_shuffle = BinaryCustomDatasetShuffle(binary_train_data,\n",
    "                                      tokenizer,\n",
    "                                      max_seq_length,\n",
    "                                      shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6588ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "        tokenizer,\n",
    "        pad_to_multiple_of=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c165342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    accr_ = metric.compute(predictions=preds, references=p.label_ids)[\"accuracy\"]\n",
    "    prec_ = metric_prec.compute(predictions=preds, references=p.label_ids)[\"precision\"]\n",
    "    recall_ = metric_recall.compute(predictions=preds, references=p.label_ids)[\"recall\"]\n",
    "    f1_ = metric_f1.compute(predictions=preds, references=p.label_ids)[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        'accuracy' : accr_,\n",
    "        'precision' : prec_,\n",
    "        'recall' : recall_,\n",
    "        'f1' : f1_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de2bd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "023dbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=8,\n",
    "#             sampler=None,\n",
    "#             collate_fn=data_collator,\n",
    "#             drop_last=False,\n",
    "#             num_workers=2,\n",
    "#             pin_memory=False\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d47b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_s = DataLoader(\n",
    "            train_dataset_shuffle,\n",
    "            batch_size=64,\n",
    "            sampler=None,\n",
    "            collate_fn=data_collator,\n",
    "            drop_last=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d5b395a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_loader\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m     cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in data_loader:\n",
    "    print(i['labels'])\n",
    "    cnt += 1\n",
    "    if cnt == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ee3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for k in data_loader_s:\n",
    "    print(k['labels'])\n",
    "    cnt += 1\n",
    "    if cnt == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55525e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3868a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35385d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5fa56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39ce27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c226f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e902be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c87bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22971baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9410982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422b223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aaf8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d734ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba34cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a205ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b07f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances, tokenizer, max_length):\n",
    "        self.instances = instances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ = [' ' + self.instances[idx]['question']] + self.instances[idx]['text']\n",
    "        input_txt =  f' { self.sep_token } '.join(input_) + ' '\n",
    "        \n",
    "        output = self.tokenizer(\n",
    "            input_txt, \n",
    "            # return_tensors=\"pt\", will be applied later through collator\n",
    "            # padding=True, will be padded later through collate\n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length)\n",
    "        \n",
    "        item = {key : val for key, val in output.items()}\n",
    "        item['labels'] = torch.tensor(self.instances[idx]['labels'])\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "args = [\"--model_name_or_path\", 'allenai/longformer-large-4096', '--output_dir', './']\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=model_args.num_labels,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    )\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.train_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    train_instance = instances[data_args.dev_size:]\n",
    "    dev_instance = instances[:data_args.dev_size]\n",
    "    \n",
    "    train_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    dev_dataset = CustomDataset(train_instance, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    instances, cut_off, total_questions = preprocessing_data(\n",
    "        data_args.test_file, \n",
    "        data_args.sample_size, \n",
    "        data_args.position)\n",
    "    \n",
    "    test_dataset = CustomDataset(instances, \n",
    "                               tokenizer, \n",
    "                               model_args.max_seq_length)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"xnli\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize Trainer\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer, \n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_train else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7e951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930ab04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.dataset_name = a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed523bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e978cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    "    )\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relevance-kilt",
   "language": "python",
   "name": "relevance-kilt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
