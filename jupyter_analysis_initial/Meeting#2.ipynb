{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1ca714",
   "metadata": {},
   "source": [
    "# Meeting 2 (standup meeting 20221116) \n",
    "- Data\n",
    "- Alternatives for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f7d42",
   "metadata": {},
   "source": [
    "## Data\n",
    "- DPR nagtive sampes : Done\n",
    "- Random Samples : Not yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d160dc",
   "metadata": {},
   "source": [
    "## Alternatives for training\n",
    "- Obstacles \n",
    "    - Applying Sequence Classification Task might not be sufficient way to measure downstream task performance\n",
    "        - Professor used downstream tasks directly\n",
    "        - only generate 1:1 sample ratio of positive & negative\n",
    "            - positive \\[query \\[sep\\] pos \\[sep\\] neg_1 \\[sep\\], ... \\]\n",
    "            - negative \\[query \\[sep\\] neg_1 \\[sep\\] neg_2 \\[sep\\], ... \\]\n",
    "    - Sometimes about 99% of sentences are under 1024 token length when 1 query + 5 sample_size\n",
    "        - but centered around 512 tokens\n",
    "        - Transformer based architecture took so long\n",
    "        \n",
    "- Alternatives\n",
    "    1. apply directly to downstream tasks since we have retrieved results \n",
    "        - For fact-verification may consider GEAR\n",
    "        - For question-answering may consider FiD\n",
    "    2. modify Longformer \n",
    "        - attention_ids to change local attention. \n",
    "        - change window size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_transformers",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
